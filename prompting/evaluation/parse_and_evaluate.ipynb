{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep, time\n",
    "from datetime import date\n",
    "import threading\n",
    "from typing import Tuple\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "from google.ai import generativelanguage as glm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to folder with inference CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_list = [\n",
    "    [\"../inference/outputs/llm_prompt_w_transcript_outputs\", \"whole\"],\n",
    "    [ \"../inference/outputs/llm_prompt_s_transcript_outputs\", \"segmentwise\"],\n",
    "    [\"../inference/outputs/mllm_s_combined_output\", \"segmentwise\"],\n",
    "    [\"../inference/outputs/mllm_w_combined_output\", \"whole\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary for whole transcript inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "global inference_dataframes_dictionary\n",
    "inference_dataframes_dictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files(whole_folder_path, prefix_s_or_w):\n",
    "    # Loop through each file in the folder\n",
    "    for filename in os.listdir(whole_folder_path):\n",
    "        if filename.endswith(\".csv\"):  # Process only CSV files\n",
    "            file_path = os.path.join(whole_folder_path, filename)\n",
    "            \n",
    "            # Read the CSV file as a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Take inference date \n",
    "            inference_date = df['date_of_inference'].iloc[0]\n",
    "\n",
    "            # Rename 'title' column to 'video_title' if it exists\n",
    "            if 'title' in df.columns:\n",
    "                df.rename(columns={'title': 'video_title'}, inplace=True)\n",
    "\n",
    "            # Get the first value from the 'model_name' column\n",
    "            if 'model_name' in df.columns and not df.empty:\n",
    "                key = df['model_name'].iloc[0]\n",
    "                #print(key)\n",
    "\n",
    "                if key == \"deepseek-ai/DeepSeek-R1\":\n",
    "                    def extract_json(text):\n",
    "                        # 1) Find the end of the </think> tag.\n",
    "                        # If not found, we'll just use the original text.\n",
    "                        idx_after_think = text.find('</think>')\n",
    "                        if idx_after_think != -1:\n",
    "                            text = text[idx_after_think + len('</think>'):]\n",
    "\n",
    "                        # 2) Find the first '{' and the last '}' from the (trimmed) text.\n",
    "                        start_idx = text.find('{')\n",
    "                        end_idx = text.rfind('}') + 1\n",
    "\n",
    "                        # 3) Validate that we have plausible braces.\n",
    "                        if start_idx == -1 or end_idx == 0:\n",
    "                            # Means no valid JSON found\n",
    "                            return None\n",
    "\n",
    "                        # 4) Extract the substring that should be valid JSON\n",
    "                        return text[start_idx:end_idx]\n",
    "                    \n",
    "                    # Apply the function to each value in the 'prompt_output' column\n",
    "                    df['prompt_output'] = df['prompt_output'].apply(extract_json)\n",
    "                    print(\"Preprocessed DeepSeek-R1 prompt output\")\n",
    "                    \n",
    "                # Appending \"whole_\" prefix before each model name for the key (to differentiate between whole and segmentwise inference results)\n",
    "                modified_key = f\"{prefix_s_or_w}_{key}\"\n",
    "                \n",
    "                # Add the key-value pair to the dictionary\n",
    "                inference_dataframes_dictionary[modified_key] = df\n",
    "\n",
    "                print(modified_key, \"Appended to dictionary of inference results\", f\"[{inference_date}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole_Qwen/Qwen2.5-7B-Instruct-Turbo Appended to dictionary of inference results [2025-02-24]\n",
      "Preprocessed DeepSeek-R1 prompt output\n",
      "whole_deepseek-ai/DeepSeek-R1 Appended to dictionary of inference results [2025-02-24]\n",
      "whole_gemini-2.0-pro-exp-02-05 Appended to dictionary of inference results [2025-02-24]\n",
      "whole_mistralai/Mistral-7B-Instruct-v0.2 Appended to dictionary of inference results [2025-02-24]\n",
      "whole_meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo Appended to dictionary of inference results [2025-02-24]\n",
      "whole_gemini-1.5-flash-002 Appended to dictionary of inference results [2025-02-24]\n",
      "whole_mistralai/Mixtral-8x22B-Instruct-v0.1 Appended to dictionary of inference results [2025-02-24]\n",
      "whole_Qwen/Qwen2.5-72B-Instruct-Turbo Appended to dictionary of inference results [2025-02-24]\n",
      "whole_claude-3-5-sonnet-20241022 Appended to dictionary of inference results [2025-02-24]\n",
      "whole_claude-3-5-haiku-20241022 Appended to dictionary of inference results [2025-02-24]\n",
      "whole_gpt-4o-2024-08-06 Appended to dictionary of inference results [2025-02-24]\n",
      "whole_gemini-1.5-pro-002 Appended to dictionary of inference results [2025-02-24]\n",
      "whole_meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo Appended to dictionary of inference results [2025-02-24]\n",
      "whole_meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo Appended to dictionary of inference results [2025-02-24]\n",
      "whole_deepseek-ai/DeepSeek-V3 Appended to dictionary of inference results [2025-02-24]\n",
      "whole_gemini-2.0-flash-001 Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_Qwen/Qwen2.5-7B-Instruct-Turbo Appended to dictionary of inference results [2025-02-24]\n",
      "Preprocessed DeepSeek-R1 prompt output\n",
      "segmentwise_deepseek-ai/DeepSeek-R1 Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_gemini-2.0-pro-exp-02-05 Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_mistralai/Mistral-7B-Instruct-v0.2 Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_gemini-1.5-flash-002 Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_mistralai/Mixtral-8x22B-Instruct-v0.1 Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_Qwen/Qwen2.5-72B-Instruct-Turbo Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_claude-3-5-sonnet-20241022 Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_claude-3-5-haiku-20241022 Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_gpt-4o-2024-08-06 Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_gemini-1.5-pro-002 Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_deepseek-ai/DeepSeek-V3 Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_gemini-2.0-flash-001 Appended to dictionary of inference results [2025-02-24]\n",
      "segmentwise_gemini-1.5-flash-002_outputs_video_segments Appended to dictionary of inference results [nan]\n",
      "segmentwise_gpt-4o-2024-08-06_outputs_video_segments Appended to dictionary of inference results [nan]\n",
      "segmentwise_llava-v1.6-mistral-7b-hf_outputs_video_segments Appended to dictionary of inference results [nan]\n",
      "segmentwise_gemini-2.0-flash-001_outputs_video_segments Appended to dictionary of inference results [nan]\n",
      "segmentwise_gemini-1.5-pro-002_outputs_video_segments Appended to dictionary of inference results [nan]\n",
      "segmentwise_gemini-2.0-pro-exp-02-05_outputs_video_segments Appended to dictionary of inference results [nan]\n",
      "whole_gemini-1.5-pro-002_outputs_video_full_length Appended to dictionary of inference results [nan]\n",
      "whole_gemini-2.0-pro-exp-02-05_outputs_video_full_length Appended to dictionary of inference results [nan]\n",
      "whole_gemini-1.5-flash-002_outputs_video_full_length Appended to dictionary of inference results [nan]\n",
      "whole_gpt-4o-2024-08-06_outputs_video_full_length Appended to dictionary of inference results [nan]\n",
      "whole_gemini-2.0-flash-001_outputs_video_full_length Appended to dictionary of inference results [nan]\n"
     ]
    }
   ],
   "source": [
    "for folder_path, s_or_w in folders_list:\n",
    "    process_csv_files(folder_path, s_or_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary for inference results\n",
    "Here key is model_name and value is the dictionary itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a list of tuples. \n",
    "\n",
    "Tuple first element: folder path to folder with csvs\n",
    "Tuple second element: string which has \"whole\" or \"segmentwise\" value\n",
    "I will iterate over the list of tuples\n",
    "\n",
    "Every iteration, you call the processing csv function, give it the folder path and the other string value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inference_dataframes_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole_Qwen/Qwen2.5-7B-Instruct-Turbo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>transcript</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_name</th>\n",
       "      <th>date_of_inference</th>\n",
       "      <th>prompt_used</th>\n",
       "      <th>prompt_output</th>\n",
       "      <th>video_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0CJU8R4oNFk</td>\n",
       "      <td>5 Stocks to Buy Now to Double Your Money</td>\n",
       "      <td>Hey Bowtie Nation, Joseph Hogue here with the...</td>\n",
       "      <td>Together AI</td>\n",
       "      <td>Qwen/Qwen2.5-7B-Instruct-Turbo</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>Analyze the YouTube video transcript and video...</td>\n",
       "      <td>{\\n  \"Stock Recommendations Present\": \"Yes\",\\n...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0Fg0YsbOzJA</td>\n",
       "      <td>i am selling it</td>\n",
       "      <td>So we have decided to start selling out of Wy...</td>\n",
       "      <td>Together AI</td>\n",
       "      <td>Qwen/Qwen2.5-7B-Instruct-Turbo</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>Analyze the YouTube video transcript and video...</td>\n",
       "      <td>{\\n    \"Stock Recommendations Present\": \"Yes\",...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0OJIHD_o59M</td>\n",
       "      <td>The Best Internet Stocks for 2023 You Can Buy Now</td>\n",
       "      <td>Hey Bowtie Nation, Joseph Hogue here and a ve...</td>\n",
       "      <td>Together AI</td>\n",
       "      <td>Qwen/Qwen2.5-7B-Instruct-Turbo</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>Analyze the YouTube video transcript and video...</td>\n",
       "      <td>{\\n  \"Stock Recommendations Present\": \"Yes\",\\n...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1Gm4A7EFYI4</td>\n",
       "      <td>I Just Bought The PERFECT Dividend Stock (At A...</td>\n",
       "      <td>So I've had my eye on a few different stocks ...</td>\n",
       "      <td>Together AI</td>\n",
       "      <td>Qwen/Qwen2.5-7B-Instruct-Turbo</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>Analyze the YouTube video transcript and video...</td>\n",
       "      <td>{\\n    \"Stock Recommendations Present\": \"Yes\",...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1Lx7z_x4Rc0</td>\n",
       "      <td>ðŸ”µWARNING TO EVERYONE!!!ðŸ”µ  I JUST SOLD IT ALL!!!</td>\n",
       "      <td>Family we absolutely dominated it today. If y...</td>\n",
       "      <td>Together AI</td>\n",
       "      <td>Qwen/Qwen2.5-7B-Instruct-Turbo</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>Analyze the YouTube video transcript and video...</td>\n",
       "      <td>{\\n  \"Stock Recommendations Present\": \"Yes\",\\n...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title  \\\n",
       "0  0CJU8R4oNFk           5 Stocks to Buy Now to Double Your Money   \n",
       "1  0Fg0YsbOzJA                                    i am selling it   \n",
       "2  0OJIHD_o59M  The Best Internet Stocks for 2023 You Can Buy Now   \n",
       "3  1Gm4A7EFYI4  I Just Bought The PERFECT Dividend Stock (At A...   \n",
       "4  1Lx7z_x4Rc0    ðŸ”µWARNING TO EVERYONE!!!ðŸ”µ  I JUST SOLD IT ALL!!!   \n",
       "\n",
       "                                          transcript model_source  \\\n",
       "0   Hey Bowtie Nation, Joseph Hogue here with the...  Together AI   \n",
       "1   So we have decided to start selling out of Wy...  Together AI   \n",
       "2   Hey Bowtie Nation, Joseph Hogue here and a ve...  Together AI   \n",
       "3   So I've had my eye on a few different stocks ...  Together AI   \n",
       "4   Family we absolutely dominated it today. If y...  Together AI   \n",
       "\n",
       "                       model_name date_of_inference  \\\n",
       "0  Qwen/Qwen2.5-7B-Instruct-Turbo        2025-02-24   \n",
       "1  Qwen/Qwen2.5-7B-Instruct-Turbo        2025-02-24   \n",
       "2  Qwen/Qwen2.5-7B-Instruct-Turbo        2025-02-24   \n",
       "3  Qwen/Qwen2.5-7B-Instruct-Turbo        2025-02-24   \n",
       "4  Qwen/Qwen2.5-7B-Instruct-Turbo        2025-02-24   \n",
       "\n",
       "                                         prompt_used  \\\n",
       "0  Analyze the YouTube video transcript and video...   \n",
       "1  Analyze the YouTube video transcript and video...   \n",
       "2  Analyze the YouTube video transcript and video...   \n",
       "3  Analyze the YouTube video transcript and video...   \n",
       "4  Analyze the YouTube video transcript and video...   \n",
       "\n",
       "                                       prompt_output  video_source  \n",
       "0  {\\n  \"Stock Recommendations Present\": \"Yes\",\\n...           NaN  \n",
       "1  {\\n    \"Stock Recommendations Present\": \"Yes\",...           NaN  \n",
       "2  {\\n  \"Stock Recommendations Present\": \"Yes\",\\n...           NaN  \n",
       "3  {\\n    \"Stock Recommendations Present\": \"Yes\",...           NaN  \n",
       "4  {\\n  \"Stock Recommendations Present\": \"Yes\",\\n...           NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_key_name = next(iter(inference_dataframes_dictionary), None)\n",
    "print(first_key_name)\n",
    "inference_dataframes_dictionary[first_key_name].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference_dataframes_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM output parsing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_output(df_llm_raw_output, model_name):\n",
    "\n",
    "    log_stream = StringIO()\n",
    "\n",
    "    # Open a log file for writing\n",
    "    def log_print(*args, **kwargs):\n",
    "        \"\"\"Helper function to log and print simultaneously.\"\"\"\n",
    "        print(*args, **kwargs)\n",
    "        print(*args, **kwargs, file=log_stream)\n",
    "\n",
    "    \n",
    "    log_print(model_name)\n",
    "\n",
    "    if model_name.startswith(\"whole_\"):\n",
    "        recommendations_key = \"Recommendations\" # CHANGED\n",
    "    elif model_name.startswith(\"segmentwise_\"):\n",
    "        recommendations_key = \"Recommendation\" # CHANGED\n",
    "    else:\n",
    "        log_print(\"Unknown model name prefix. Defaulting to 'Recommendations'\") # CHANGED\n",
    "        recommendations_key = \"Recommendations\" # CHANGED\n",
    "\n",
    "    \n",
    "    # Lists to store indices of valid and invalid JSON strings\n",
    "    valid_json_indices = []\n",
    "    invalid_json_indices = []\n",
    "    no_recommendation_in_valid_indices = []  # New list to store indices with empty recommendations\n",
    "\n",
    "\n",
    "    # Check each JSON string in the DataFrame\n",
    "    for i, data_str in enumerate(df_llm_raw_output['prompt_output']):\n",
    "\n",
    "        # First, check if the cell is empty or only contains whitespace\n",
    "        if not isinstance(data_str, str) or not data_str.strip():\n",
    "            # If empty (or None, or not a string), mark it as invalid JSON\n",
    "            invalid_json_indices.append(i)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            json.loads(data_str)  # Attempt to parse\n",
    "            valid_json_indices.append(i)  # Add to valid indices if successful\n",
    "        except json.JSONDecodeError:\n",
    "            invalid_json_indices.append(i)  # Add to invalid indices if there's an error\n",
    "\n",
    "    # Print results\n",
    "    if invalid_json_indices:\n",
    "        log_print(\"Invalid JSON strings found at the following indices:\")\n",
    "        for index in invalid_json_indices:\n",
    "            log_print(f\"Index {index}:\")\n",
    "            #print(df_llm_raw_output['prompt_output'][index])\n",
    "    else:\n",
    "        log_print(\"All JSON strings are valid!\")\n",
    "    \n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # List to store DataFrames for each valid JSON entry\n",
    "    recommendations_with_metadata = []\n",
    "\n",
    "    # Loop through each valid JSON index\n",
    "    for index in valid_json_indices:\n",
    "        # Get the JSON string at the current index\n",
    "        data_str = df_llm_raw_output['prompt_output'][index]\n",
    "        \n",
    "        try:\n",
    "            # Parse the JSON string\n",
    "            data_dict = json.loads(data_str)\n",
    "\n",
    "\n",
    "            # -------------------\n",
    "            \n",
    "            # Check if the recommendations list is empty\n",
    "            if not data_dict.get(recommendations_key, []):  # Default to empty list if key doesn't exist\n",
    "                no_recommendation_in_valid_indices.append(index)  # Track this index\n",
    "                continue\n",
    "            \n",
    "            # -------------------\n",
    "\n",
    "            # Convert the recommendations list into a DataFrame\n",
    "            recommendations_df = pd.DataFrame(data_dict[recommendations_key])\n",
    "            \n",
    "            # Add metadata columns from df_llm_raw_output to recommendations_df\n",
    "            for col in [\"video_id\", \"video_title\", \"transcript\", \"model_source\", \"model_name\", \"date_of_inference\", \"prompt_used\"]:\n",
    "                recommendations_df[col] = df_llm_raw_output[col][index]\n",
    "            \n",
    "            # Append this DataFrame to the list\n",
    "            recommendations_with_metadata.append(recommendations_df)\n",
    "            \n",
    "        except KeyError as e:\n",
    "            log_print(f\"KeyError for index {index}: {e}\")\n",
    "        except Exception as e:\n",
    "            log_print(f\"An unexpected error occurred for index {index}: {e}\")\n",
    "\n",
    "    # Combine all the DataFrames into one\n",
    "    df_llm_annotations = pd.concat(recommendations_with_metadata, ignore_index=True)\n",
    "\n",
    "     # Process indices with empty recommendations\n",
    "    for index in no_recommendation_in_valid_indices:\n",
    "        # Create a new DataFrame with metadata columns filled\n",
    "        metadata_row = {\n",
    "            \"video_id\": df_llm_raw_output[\"video_id\"][index],\n",
    "            \"video_title\": df_llm_raw_output[\"video_title\"][index],\n",
    "            \"transcript\": df_llm_raw_output[\"transcript\"][index],\n",
    "            \"model_source\": df_llm_raw_output[\"model_source\"][index],\n",
    "            \"model_name\": df_llm_raw_output[\"model_name\"][index],\n",
    "            \"date_of_inference\": df_llm_raw_output[\"date_of_inference\"][index],\n",
    "            \"prompt_used\": df_llm_raw_output[\"prompt_used\"][index],\n",
    "        }\n",
    "        \n",
    "        # Add columns for non-metadata as empty values\n",
    "        other_columns = {col: None for col in df_llm_annotations.columns if col not in metadata_row}\n",
    "        \n",
    "        # Combine metadata and other columns\n",
    "        empty_recommendation_row = {**metadata_row, **other_columns}\n",
    "        \n",
    "        # Append the row to df_llm_annotations\n",
    "        df_llm_annotations = pd.concat(\n",
    "            [df_llm_annotations, pd.DataFrame([empty_recommendation_row])],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "\n",
    "    # Compute stats \n",
    "\n",
    "    # Stat: Number of videos in the inference dataframe    \n",
    "    num_videos_inference = df_llm_raw_output['video_id'].nunique()\n",
    "    log_print(f\"Number of videos in the inference dataframe: {num_videos_inference}\")\n",
    "\n",
    "    # Stat: Number of videos in the cleaned dataframe\n",
    "    num_videos_cleaned = df_llm_annotations['video_id'].nunique()\n",
    "    log_print(f\"Number of videos in the cleaned dataframe: {num_videos_cleaned}\")\n",
    "\n",
    "    # Stat: Differering video ids from inference (raw) and parsed (cleaned) dataframe\n",
    "    unique_raw_output = set(df_llm_raw_output['video_id'])\n",
    "    unique_annotations = set(df_llm_annotations['video_id'])\n",
    "    not_in_annotations = unique_raw_output - unique_annotations\n",
    "    not_in_raw_output = unique_annotations - unique_raw_output\n",
    "    log_print(\"video_id(s) in inference dataframe but not in final parsed dataframe:\")\n",
    "    log_print(not_in_annotations)\n",
    "    log_print(\"\\nvideo_id(s) in final parsed dataframe but not in inference dataframe:\")\n",
    "    log_print(not_in_raw_output)\n",
    "\n",
    "    # Stat: Number of indices with no recommendations\n",
    "    log_print(f\"Indicies with no recommendations: {no_recommendation_in_valid_indices}\")\n",
    "\n",
    "    # Stat: Number of valid and invalid JSON strings\n",
    "    log_print(f\"Valid JSON count: {len(valid_json_indices)}\")\n",
    "    log_print(f\"Invalid JSON count: {len(invalid_json_indices)}\")\n",
    "\n",
    "    #log_filename = f\"{model_name}_output_log.txt\"\n",
    "\n",
    "    \"\"\"with open(log_filename, \"w\") as log_file:\n",
    "        log_file.write(log_stream.getvalue())\"\"\"\n",
    "\n",
    "    return df_llm_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole_Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 288\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [48, 74, 82, 111, 122, 152, 214, 216, 217, 218, 219, 222, 223, 224, 229, 232, 240, 241, 249, 250, 251, 252, 257, 261, 264, 265, 269, 272, 275, 276, 277, 278, 279, 282]\n",
      "Valid JSON count: 288\n",
      "Invalid JSON count: 0\n",
      "whole_deepseek-ai/DeepSeek-R1\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 10:\n",
      "Index 13:\n",
      "Index 44:\n",
      "Index 73:\n",
      "Index 156:\n",
      "Index 195:\n",
      "Index 209:\n",
      "Index 212:\n",
      "Index 265:\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 279\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'wpOISCIl4Cw', 'sQRH-lQljQw', '3eSDDUgC5Mo', 'ZeyBx-ItMwQ', '4J0-56UXX1M', 'aV-vPmnZBs8', 'Ql_qpfN_bls', 'ZxC771f04rQ', 'fUwNBd6VBjo'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [48, 181, 216, 218, 219, 222, 223, 232, 241, 242, 249, 250, 251, 252, 259, 264, 269, 272, 276, 278, 279, 280, 285]\n",
      "Valid JSON count: 279\n",
      "Invalid JSON count: 9\n",
      "whole_gemini-2.0-pro-exp-02-05\n",
      "All JSON strings are valid!\n",
      "An unexpected error occurred for index 1: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 3: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 4: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 7: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 9: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 10: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 13: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 15: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 16: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 18: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 19: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 25: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 28: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 30: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 31: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 34: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 36: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 38: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 39: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 40: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 41: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 43: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 44: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 48: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 49: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 53: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 54: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 66: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 67: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 68: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 71: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 72: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 73: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 82: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 87: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 88: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 89: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 91: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 92: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 93: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 95: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 102: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 104: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 106: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 108: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 117: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 120: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 126: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 128: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 133: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 138: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 139: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 142: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 146: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 148: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 149: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 150: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 161: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 163: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 165: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 168: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 170: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 174: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 176: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 184: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 186: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 187: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 190: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 195: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 196: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 199: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 201: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 208: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 209: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 211: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 212: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 215: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 217: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 224: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 227: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 228: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 231: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 237: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 240: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 242: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 243: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 246: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 253: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 255: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 256: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 258: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 259: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 260: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 262: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 265: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 266: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 268: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 275: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 282: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 284: 'list' object has no attribute 'get'\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 188\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'JG_qBhUrcjY', 'PY-s6Rfjixk', '4z3HW66MlZg', 'G_ImDM9XekI', 'tSwJAruVKeA', 'fSTQKEo-GeI', '9y5Gx0EFpD0', 'QzAkU62iSho', 'QT5yZMJmtRk', 'Ql_qpfN_bls', '8dWiBE2npfE', 'QAg8sF18CME', '2SXTAzUFA6E', '6OcqohZkiAE', 'w52eXQwD8LA', 'AdNL7gyuoAA', '7UEnvCoGa1w', 'fQ9kyieXiGw', 'XDnFv3YlgqE', 'ZxC771f04rQ', '5SHHc26ielg', 'rLM2PEmoqxM', 'Tnd9L4Gt3qM', 'HgxUt4rZ8nY', '0Fg0YsbOzJA', 'fSqtmSNB_TA', 'Jy1uTayq-EE', 'pGLnC0jEfhI', 'P3oXSKZXfXA', 'ZeyBx-ItMwQ', 'I2UEeE8IWTA', 'ACFgvMnC0I4', 'aQZE7_di4Us', 'J0i5PDTB1JQ', 'c0oJeieUTJA', '9O1yg4UFsrU', 'opxXRantWpc', 'hZiD6pkTLRc', '9etNUs9C0C4', '8KsyL_aULaI', 'hOreMQuTaAc', '1Lx7z_x4Rc0', 'H5O59znepLA', 'n2tknkRqQUM', '2saTFHyw_jU', 'bJds-IjfBtc', 'xFQWbRS0cps', 'wdmILBs5awA', 'v8GMzfstic0', '3eSDDUgC5Mo', 'e_oapzidNtU', 'H__6eTsas2E', 'J2OpAmyr_JY', 'Id6BvoYQFjs', 'zjD4WtL_SO4', 'CgbU0ZQDfxc', 'LjuvlDub3Mw', 'wx9FCo71jqY', 'PZ_vkfyu8Ls', 'METDeKbXaaQ', 'aV-vPmnZBs8', 'bK0Wp9_pqh0', 'dCiCilcUYwk', 'Y8iGpTUtrvI', 'M34H7pUKcUo', 'hItddtAJW7Y', 'caRFs94Qqtw', 'U0EgnrhVvaM', 'QWupFI5Q9Ls', 'HFva07tHj6k', 'rpw2rb1XEZE', 'fUwNBd6VBjo', 'VwnyBBfRtvU', 'wpOISCIl4Cw', 'enBqeyYe9Fo', 'aDd89JaLBqE', 'dm8n8UDiE1E', 'w2QxUEwwJIE', '1Gm4A7EFYI4', 'Hr8LR_OGLys', '5KLQl4nqJTo', 'EbwRE3CcU9k', '4J0-56UXX1M', 'UlWLNbMPNvM', '6oYz5Dobnm4', 'vh2ZgvmXfXc', 'rk3I-v4aG1Q', 'mSg9B9S4aI0', 'uHsTsSznY6Q', '8jYGZPOOCWM', 'MWlli2e3eV0', 'JM0NetAtxz8', '2LqTgKgSId8', 'npKlFjOYzQo', 'XSd_PyCrRa0', 'eugmDFFXG5M', 'UsWC3G8prFk', 'M3HuI7hiM8k', 'p7pOUitM9ck', 'Za4cqqvN4UY'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [216, 219, 241, 249, 251, 269, 270, 276, 285]\n",
      "Valid JSON count: 288\n",
      "Invalid JSON count: 0\n",
      "whole_mistralai/Mistral-7B-Instruct-v0.2\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 78:\n",
      "Index 93:\n",
      "Index 100:\n",
      "Index 122:\n",
      "Index 168:\n",
      "Index 236:\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 282\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'Im_6coFrQBI', 'GctuXjR4vfA', 'I2UEeE8IWTA', 'uHsTsSznY6Q', 'maP3JdFQDBs', 'FcPSVgaLJwc'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [40, 111, 127, 132, 152, 214, 216, 219, 222, 223, 224, 232, 241, 249, 250, 251, 252, 257, 264, 265, 269, 270, 272, 275, 276, 277, 278, 282, 285]\n",
      "Valid JSON count: 282\n",
      "Invalid JSON count: 6\n",
      "whole_meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 288\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [111, 216, 219, 223, 232, 241, 249, 250, 251, 252, 264, 269, 270, 273, 276, 278, 279]\n",
      "Valid JSON count: 288\n",
      "Invalid JSON count: 0\n",
      "whole_gemini-1.5-flash-002\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 288\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [216, 219, 223, 241, 249, 250, 251, 269, 270, 276, 278, 279]\n",
      "Valid JSON count: 288\n",
      "Invalid JSON count: 0\n",
      "whole_mistralai/Mixtral-8x22B-Instruct-v0.1\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 40:\n",
      "Index 111:\n",
      "Index 152:\n",
      "Index 218:\n",
      "Index 219:\n",
      "Index 222:\n",
      "Index 223:\n",
      "Index 232:\n",
      "Index 241:\n",
      "Index 249:\n",
      "Index 250:\n",
      "Index 251:\n",
      "Index 252:\n",
      "Index 264:\n",
      "Index 269:\n",
      "Index 272:\n",
      "Index 275:\n",
      "Index 277:\n",
      "Index 278:\n",
      "Index 279:\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 268\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'keikMNOJ5ho', 'sfEdPdJ0Woo', 'EEveneAnfic', 'BhyqKH0-uoY', 'lYRaZHd3UW4', 'x3pBWDsmd8M', 'Px-Gu9R8hco', 'sZYNdjVEYbY', 'lri8U8grQXI', 'afVJEkhqTzA', 'l10weo3BmEo', '7yFAePHB8GA', 'aDd89JaLBqE', 's4hmXuWc8Oc', 'wxZ_lxybdME', 'Hdn49_EqGLI', 'wmp2_ZxfrBk', 'UsWC3G8prFk', '7uIMOojp4wc', 'kuUGlnqxJxU'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [216, 276]\n",
      "Valid JSON count: 268\n",
      "Invalid JSON count: 20\n",
      "whole_Qwen/Qwen2.5-72B-Instruct-Turbo\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 288\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [111, 216, 219, 222, 223, 232, 241, 249, 251, 252, 264, 269, 272, 275, 276, 277, 278, 279]\n",
      "Valid JSON count: 288\n",
      "Invalid JSON count: 0\n",
      "whole_claude-3-5-sonnet-20241022\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 288\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [216, 218, 219, 222, 223, 232, 241, 249, 250, 251, 252, 264, 269, 270, 272, 273, 275, 276, 285]\n",
      "Valid JSON count: 288\n",
      "Invalid JSON count: 0\n",
      "whole_claude-3-5-haiku-20241022\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 288\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [40, 111, 147, 216, 217, 219, 222, 223, 232, 241, 249, 250, 251, 252, 264, 269, 270, 272, 273, 275, 276, 278, 279, 282]\n",
      "Valid JSON count: 288\n",
      "Invalid JSON count: 0\n",
      "whole_gpt-4o-2024-08-06\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 288\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [137, 181, 216, 218, 219, 220, 222, 223, 229, 232, 241, 249, 250, 251, 252, 261, 264, 266, 269, 270, 271, 272, 273, 275, 276, 278, 279]\n",
      "Valid JSON count: 288\n",
      "Invalid JSON count: 0\n",
      "whole_gemini-1.5-pro-002\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 288\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [111, 216, 219, 223, 241, 249, 250, 251, 269, 272, 273, 276, 278, 279, 285]\n",
      "Valid JSON count: 288\n",
      "Invalid JSON count: 0\n",
      "whole_meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 226:\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 287\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'cQmJW_7WCt0'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [216, 219, 223, 232, 241, 249, 250, 251, 269, 275, 276, 278]\n",
      "Valid JSON count: 287\n",
      "Invalid JSON count: 1\n",
      "whole_meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 104:\n",
      "Index 111:\n",
      "Index 156:\n",
      "Index 159:\n",
      "Index 163:\n",
      "Index 258:\n",
      "Index 272:\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 281\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'JG_qBhUrcjY', 'keikMNOJ5ho', 'sQRH-lQljQw', 'tSwJAruVKeA', 'T5N-F5Hswz0', 'sZYNdjVEYbY', 'opxXRantWpc'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [122, 216, 219, 223, 232, 241, 249, 250, 251, 252, 261, 264, 269, 273, 276, 278, 282, 285]\n",
      "Valid JSON count: 281\n",
      "Invalid JSON count: 7\n",
      "whole_deepseek-ai/DeepSeek-V3\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 26:\n",
      "Index 29:\n",
      "Index 40:\n",
      "Index 53:\n",
      "Index 57:\n",
      "Index 58:\n",
      "Index 60:\n",
      "Index 76:\n",
      "Index 81:\n",
      "Index 137:\n",
      "Index 170:\n",
      "Index 173:\n",
      "Index 188:\n",
      "Index 215:\n",
      "Index 223:\n",
      "Index 244:\n",
      "Index 247:\n",
      "Index 275:\n",
      "Index 283:\n",
      "Index 286:\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 268\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'_KdRpU1CMzY', '87EsTw_ZnH8', 'BhyqKH0-uoY', 'UlWLNbMPNvM', 'c0oJeieUTJA', '8Fh6LT9JTmE', 'GQn5nXyyUM0', 'peKt0aztFqg', 'd6LDF-TEUvI', 'dlKY5YfZdpU', 'gA7oOVI4-wI', '2saTFHyw_jU', 'Y61LJ2Pwsgw', 'I8zBRxZVdAM', 'dD9zKscgRqo', 'aDd89JaLBqE', 'UsWC3G8prFk', 'KEYiBTNqJ48', 'wCBK1DteQrE', 'v534SoqNeUA'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [111, 216, 219, 222, 232, 241, 249, 250, 251, 252, 261, 264, 269, 270, 272, 273, 276, 277, 278, 279]\n",
      "Valid JSON count: 268\n",
      "Invalid JSON count: 20\n",
      "whole_gemini-2.0-flash-001\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 288\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [216, 219, 223, 232, 241, 249, 250, 251, 252, 269, 272, 276, 278]\n",
      "Valid JSON count: 288\n",
      "Invalid JSON count: 0\n",
      "segmentwise_Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [12, 19, 20, 84, 90, 91, 153, 154, 160, 166, 193, 226, 250, 267, 274, 301, 321, 355]\n",
      "Valid JSON count: 381\n",
      "Invalid JSON count: 0\n",
      "segmentwise_deepseek-ai/DeepSeek-R1\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 22:\n",
      "Index 36:\n",
      "Index 59:\n",
      "Index 78:\n",
      "Index 79:\n",
      "Index 154:\n",
      "Index 302:\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [6, 32, 50, 84, 91, 133, 167, 202, 225, 226, 256, 257, 320, 322]\n",
      "Valid JSON count: 374\n",
      "Invalid JSON count: 7\n",
      "segmentwise_gemini-2.0-pro-exp-02-05\n",
      "All JSON strings are valid!\n",
      "An unexpected error occurred for index 7: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 18: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 20: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 21: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 22: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 24: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 36: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 37: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 38: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 39: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 41: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 62: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 75: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 76: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 78: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 79: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 81: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 91: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 93: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 98: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 105: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 106: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 117: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 128: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 133: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 137: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 145: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 147: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 150: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 161: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 166: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 191: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 195: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 203: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 209: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 213: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 215: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 219: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 225: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 230: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 233: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 238: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 244: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 248: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 252: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 270: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 278: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 288: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 298: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 300: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 302: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 306: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 312: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 315: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 317: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 324: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 325: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 330: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 332: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 333: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 334: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 335: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 336: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 337: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 356: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 360: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 367: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 369: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 377: 'list' object has no attribute 'get'\n",
      "An unexpected error occurred for index 379: 'list' object has no attribute 'get'\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 141\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'E3VlixmyPGU', '8jYGZPOOCWM', 'wniiVmbSAcM', 'xscxbcxEHcg', 'dsutyRIcE60', 'wL2SjDtvu7U', 'tRsZJshhduY', 'mvFymoZTMNA', 'lXz6ph7MfdU'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [193]\n",
      "Valid JSON count: 381\n",
      "Invalid JSON count: 0\n",
      "segmentwise_mistralai/Mistral-7B-Instruct-v0.2\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 34:\n",
      "Index 82:\n",
      "Index 123:\n",
      "Index 133:\n",
      "Index 159:\n",
      "Index 171:\n",
      "Index 180:\n",
      "Index 183:\n",
      "Index 224:\n",
      "Index 226:\n",
      "Index 255:\n",
      "Index 290:\n",
      "Index 292:\n",
      "Index 305:\n",
      "Index 331:\n",
      "Index 339:\n",
      "Index 342:\n",
      "Index 344:\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 140\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'nkYiy94RT8o', 'aV1005n5VZU', 'hItddtAJW7Y', '87EsTw_ZnH8', 'iLrnZdvU-T0', 'WndD1Oa_SM4', 'UOcobnZI7b0', 'I2UEeE8IWTA', 'vosEIobW5gk', 'uEChFtOqGeQ'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [91, 193]\n",
      "Valid JSON count: 363\n",
      "Invalid JSON count: 18\n",
      "segmentwise_meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [84, 91, 133, 193, 250, 267, 269]\n",
      "Valid JSON count: 381\n",
      "Invalid JSON count: 0\n",
      "segmentwise_gemini-1.5-flash-002\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [32, 50, 193, 250, 267, 269]\n",
      "Valid JSON count: 381\n",
      "Invalid JSON count: 0\n",
      "segmentwise_mistralai/Mixtral-8x22B-Instruct-v0.1\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 91:\n",
      "Index 152:\n",
      "Index 193:\n",
      "Index 201:\n",
      "Index 250:\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 149\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'keikMNOJ5ho'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: []\n",
      "Valid JSON count: 376\n",
      "Invalid JSON count: 5\n",
      "segmentwise_Qwen/Qwen2.5-72B-Instruct-Turbo\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [91, 154, 193, 250, 269]\n",
      "Valid JSON count: 381\n",
      "Invalid JSON count: 0\n",
      "segmentwise_claude-3-5-sonnet-20241022\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [32, 50, 84, 91, 133, 154, 193, 201, 257, 269, 339]\n",
      "Valid JSON count: 381\n",
      "Invalid JSON count: 0\n",
      "segmentwise_claude-3-5-haiku-20241022\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [5, 91, 133, 160, 193, 227, 245, 250, 251]\n",
      "Valid JSON count: 381\n",
      "Invalid JSON count: 0\n",
      "segmentwise_gpt-4o-2024-08-06\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [5, 6, 32, 50, 154, 193, 201, 202, 250, 257, 267, 268, 269, 339]\n",
      "Valid JSON count: 381\n",
      "Invalid JSON count: 0\n",
      "segmentwise_gemini-1.5-pro-002\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 266:\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [6, 32, 50, 133, 193, 201, 257, 267, 339]\n",
      "Valid JSON count: 380\n",
      "Invalid JSON count: 1\n",
      "segmentwise_meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [91, 154, 193, 250, 267, 269, 320, 321]\n",
      "Valid JSON count: 381\n",
      "Invalid JSON count: 0\n",
      "segmentwise_meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 94:\n",
      "Index 171:\n",
      "Index 201:\n",
      "Index 241:\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 148\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'qnc4_ZYWCPw', 'I2UEeE8IWTA'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [91, 154, 193, 250]\n",
      "Valid JSON count: 377\n",
      "Invalid JSON count: 4\n",
      "segmentwise_deepseek-ai/DeepSeek-V3\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 55:\n",
      "Index 198:\n",
      "Index 252:\n",
      "Index 276:\n",
      "Index 330:\n",
      "Index 331:\n",
      "Index 356:\n",
      "Index 359:\n",
      "Index 369:\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 147\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'WndD1Oa_SM4', 'wL2SjDtvu7U', 'xrJkNaH5hUk'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [32, 91, 250]\n",
      "Valid JSON count: 372\n",
      "Invalid JSON count: 9\n",
      "segmentwise_gemini-2.0-flash-001\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [193, 252, 267]\n",
      "Valid JSON count: 381\n",
      "Invalid JSON count: 0\n",
      "segmentwise_gemini-1.5-flash-002_outputs_video_segments\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [291]\n",
      "Valid JSON count: 380\n",
      "Invalid JSON count: 0\n",
      "segmentwise_gpt-4o-2024-08-06_outputs_video_segments\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 192:\n",
      "Index 343:\n",
      "Index 353:\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [274, 340, 350, 366]\n",
      "Valid JSON count: 377\n",
      "Invalid JSON count: 3\n",
      "segmentwise_llava-v1.6-mistral-7b-hf_outputs_video_segments\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 8:\n",
      "Index 15:\n",
      "Index 16:\n",
      "Index 49:\n",
      "Index 83:\n",
      "Index 110:\n",
      "Index 118:\n",
      "Index 128:\n",
      "Index 133:\n",
      "Index 135:\n",
      "Index 140:\n",
      "Index 146:\n",
      "Index 151:\n",
      "Index 159:\n",
      "Index 172:\n",
      "Index 175:\n",
      "Index 193:\n",
      "Index 196:\n",
      "Index 200:\n",
      "Index 205:\n",
      "Index 211:\n",
      "Index 223:\n",
      "Index 231:\n",
      "Index 241:\n",
      "Index 256:\n",
      "Index 260:\n",
      "Index 269:\n",
      "Index 275:\n",
      "Index 276:\n",
      "Index 287:\n",
      "Index 299:\n",
      "Index 302:\n",
      "Index 304:\n",
      "Index 311:\n",
      "Index 312:\n",
      "Number of videos in the inference dataframe: 100\n",
      "Number of videos in the cleaned dataframe: 93\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'K0AQK4bQKiQ', 'dlKY5YfZdpU', 'sQRH-lQljQw', 'WcO-Tmpb6Cc', 'c0oJeieUTJA', 'bJds-IjfBtc', 'lXz6ph7MfdU'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [1, 2, 3, 5, 7, 9, 18, 21, 22, 25, 28, 29, 35, 40, 42, 48, 55, 61, 62, 63, 65, 66, 68, 70, 71, 72, 75, 88, 91, 93, 95, 98, 100, 102, 104, 105, 109, 115, 116, 120, 121, 122, 123, 130, 131, 132, 136, 137, 141, 142, 149, 154, 157, 164, 166, 168, 170, 171, 173, 177, 181, 183, 184, 187, 189, 191, 195, 199, 203, 204, 206, 209, 216, 217, 229, 238, 245, 248, 249, 251, 257, 258, 261, 263, 264, 266, 268, 270, 278, 282, 289, 290, 305, 306, 314]\n",
      "Valid JSON count: 280\n",
      "Invalid JSON count: 35\n",
      "segmentwise_gemini-2.0-flash-001_outputs_video_segments\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [291]\n",
      "Valid JSON count: 380\n",
      "Invalid JSON count: 0\n",
      "segmentwise_gemini-1.5-pro-002_outputs_video_segments\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 22:\n",
      "Index 282:\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 148\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'E3VlixmyPGU', 'WndD1Oa_SM4'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [362]\n",
      "Valid JSON count: 374\n",
      "Invalid JSON count: 2\n",
      "segmentwise_gemini-2.0-pro-exp-02-05_outputs_video_segments\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 150\n",
      "Number of videos in the cleaned dataframe: 150\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: []\n",
      "Valid JSON count: 380\n",
      "Invalid JSON count: 0\n",
      "whole_gemini-1.5-pro-002_outputs_video_full_length\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 285\n",
      "Number of videos in the cleaned dataframe: 285\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [22, 55, 73, 132, 151, 204, 224, 242, 245, 260]\n",
      "Valid JSON count: 285\n",
      "Invalid JSON count: 0\n",
      "whole_gemini-2.0-pro-exp-02-05_outputs_video_full_length\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 288\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [55, 132, 151, 205, 244, 262]\n",
      "Valid JSON count: 288\n",
      "Invalid JSON count: 0\n",
      "whole_gemini-1.5-flash-002_outputs_video_full_length\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 286\n",
      "Number of videos in the cleaned dataframe: 286\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [22, 36, 51, 55, 132, 150, 168, 204, 207, 230, 242, 243, 245, 260]\n",
      "Valid JSON count: 286\n",
      "Invalid JSON count: 0\n",
      "whole_gpt-4o-2024-08-06_outputs_video_full_length\n",
      "Invalid JSON strings found at the following indices:\n",
      "Index 29:\n",
      "Index 119:\n",
      "Index 135:\n",
      "Index 146:\n",
      "Index 149:\n",
      "Index 282:\n",
      "Number of videos in the inference dataframe: 288\n",
      "Number of videos in the cleaned dataframe: 282\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "{'JG_qBhUrcjY', '2SXTAzUFA6E', 'Vr95CB8kK6Y', '3eSDDUgC5Mo', 'PJYV7qOdjnc', '9etNUs9C0C4'}\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [22, 51, 55, 73, 83, 85, 132, 151, 152, 169, 179, 199, 205, 213, 226, 232, 244, 247, 256, 261, 262]\n",
      "Valid JSON count: 282\n",
      "Invalid JSON count: 6\n",
      "whole_gemini-2.0-flash-001_outputs_video_full_length\n",
      "All JSON strings are valid!\n",
      "Number of videos in the inference dataframe: 285\n",
      "Number of videos in the cleaned dataframe: 285\n",
      "video_id(s) in inference dataframe but not in final parsed dataframe:\n",
      "set()\n",
      "\n",
      "video_id(s) in final parsed dataframe but not in inference dataframe:\n",
      "set()\n",
      "Indicies with no recommendations: [51, 55, 132, 151, 241, 259]\n",
      "Valid JSON count: 285\n",
      "Invalid JSON count: 0\n"
     ]
    }
   ],
   "source": [
    "parsed_inference_dataframes_dictionary = {}\n",
    "\n",
    "for model_name, df_raw in inference_dataframes_dictionary.items():\n",
    "    parsed_inference_dataframes_dictionary[model_name] = clean_llm_output(df_raw, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>Justification</th>\n",
       "      <th>Conviction Score</th>\n",
       "      <th>Ticker Name</th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>transcript</th>\n",
       "      <th>model_source</th>\n",
       "      <th>model_name</th>\n",
       "      <th>date_of_inference</th>\n",
       "      <th>prompt_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Buy</td>\n",
       "      <td>The stock is expected to return 120% over the ...</td>\n",
       "      <td>3</td>\n",
       "      <td>VERI</td>\n",
       "      <td>0CJU8R4oNFk</td>\n",
       "      <td>5 Stocks to Buy Now to Double Your Money</td>\n",
       "      <td>Hey Bowtie Nation, Joseph Hogue here with the...</td>\n",
       "      <td>Together AI</td>\n",
       "      <td>Qwen/Qwen2.5-7B-Instruct-Turbo</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>Analyze the YouTube video transcript and video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buy</td>\n",
       "      <td>The stock is expected to return 82% over the n...</td>\n",
       "      <td>3</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>0CJU8R4oNFk</td>\n",
       "      <td>5 Stocks to Buy Now to Double Your Money</td>\n",
       "      <td>Hey Bowtie Nation, Joseph Hogue here with the...</td>\n",
       "      <td>Together AI</td>\n",
       "      <td>Qwen/Qwen2.5-7B-Instruct-Turbo</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>Analyze the YouTube video transcript and video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buy</td>\n",
       "      <td>The speaker is confident in the company's grow...</td>\n",
       "      <td>3</td>\n",
       "      <td>MITK</td>\n",
       "      <td>0CJU8R4oNFk</td>\n",
       "      <td>5 Stocks to Buy Now to Double Your Money</td>\n",
       "      <td>Hey Bowtie Nation, Joseph Hogue here with the...</td>\n",
       "      <td>Together AI</td>\n",
       "      <td>Qwen/Qwen2.5-7B-Instruct-Turbo</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>Analyze the YouTube video transcript and video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Buy</td>\n",
       "      <td>The speaker is confident in the company's grow...</td>\n",
       "      <td>3</td>\n",
       "      <td>KRMD</td>\n",
       "      <td>0CJU8R4oNFk</td>\n",
       "      <td>5 Stocks to Buy Now to Double Your Money</td>\n",
       "      <td>Hey Bowtie Nation, Joseph Hogue here with the...</td>\n",
       "      <td>Together AI</td>\n",
       "      <td>Qwen/Qwen2.5-7B-Instruct-Turbo</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>Analyze the YouTube video transcript and video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buy</td>\n",
       "      <td>The speaker is confident in the company's grow...</td>\n",
       "      <td>3</td>\n",
       "      <td>IRMD</td>\n",
       "      <td>0CJU8R4oNFk</td>\n",
       "      <td>5 Stocks to Buy Now to Double Your Money</td>\n",
       "      <td>Hey Bowtie Nation, Joseph Hogue here with the...</td>\n",
       "      <td>Together AI</td>\n",
       "      <td>Qwen/Qwen2.5-7B-Instruct-Turbo</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>Analyze the YouTube video transcript and video...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Action                                      Justification Conviction Score  \\\n",
       "0    Buy  The stock is expected to return 120% over the ...                3   \n",
       "1    Buy  The stock is expected to return 82% over the n...                3   \n",
       "2    Buy  The speaker is confident in the company's grow...                3   \n",
       "3    Buy  The speaker is confident in the company's grow...                3   \n",
       "4    Buy  The speaker is confident in the company's grow...                3   \n",
       "\n",
       "  Ticker Name     video_id                               video_title  \\\n",
       "0        VERI  0CJU8R4oNFk  5 Stocks to Buy Now to Double Your Money   \n",
       "1        ZYXI  0CJU8R4oNFk  5 Stocks to Buy Now to Double Your Money   \n",
       "2        MITK  0CJU8R4oNFk  5 Stocks to Buy Now to Double Your Money   \n",
       "3        KRMD  0CJU8R4oNFk  5 Stocks to Buy Now to Double Your Money   \n",
       "4        IRMD  0CJU8R4oNFk  5 Stocks to Buy Now to Double Your Money   \n",
       "\n",
       "                                          transcript model_source  \\\n",
       "0   Hey Bowtie Nation, Joseph Hogue here with the...  Together AI   \n",
       "1   Hey Bowtie Nation, Joseph Hogue here with the...  Together AI   \n",
       "2   Hey Bowtie Nation, Joseph Hogue here with the...  Together AI   \n",
       "3   Hey Bowtie Nation, Joseph Hogue here with the...  Together AI   \n",
       "4   Hey Bowtie Nation, Joseph Hogue here with the...  Together AI   \n",
       "\n",
       "                       model_name date_of_inference  \\\n",
       "0  Qwen/Qwen2.5-7B-Instruct-Turbo        2025-02-24   \n",
       "1  Qwen/Qwen2.5-7B-Instruct-Turbo        2025-02-24   \n",
       "2  Qwen/Qwen2.5-7B-Instruct-Turbo        2025-02-24   \n",
       "3  Qwen/Qwen2.5-7B-Instruct-Turbo        2025-02-24   \n",
       "4  Qwen/Qwen2.5-7B-Instruct-Turbo        2025-02-24   \n",
       "\n",
       "                                         prompt_used  \n",
       "0  Analyze the YouTube video transcript and video...  \n",
       "1  Analyze the YouTube video transcript and video...  \n",
       "2  Analyze the YouTube video transcript and video...  \n",
       "3  Analyze the YouTube video transcript and video...  \n",
       "4  Analyze the YouTube video transcript and video...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_inference_dataframes_dictionary[first_key_name].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(inference_dataframes_dictionary))\n",
    "print(len(parsed_inference_dataframes_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename specified columns in each dataframe within the dictionary\n",
    "for key, df in parsed_inference_dataframes_dictionary.items():\n",
    "    parsed_inference_dataframes_dictionary[key] = df.rename(\n",
    "        columns={\n",
    "            \"Action\": \"llm_action\",\n",
    "            \"Conviction Score\": \"llm_conviction_score\",\n",
    "            \"Ticker Name\": \"llm_ticker_name\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output parsed CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'cleaned_csvs' subdirectory exists\n",
    "output_dir = \"cleaned_inference_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned_inference_results/cleaned_whole_Qwen_Qwen2.5-7B-Instruct-Turbo.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_deepseek-ai_DeepSeek-R1.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_gemini-2.0-pro-exp-02-05.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_mistralai_Mistral-7B-Instruct-v0.2.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_meta-llama_Meta-Llama-3.1-405B-Instruct-Turbo.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_gemini-1.5-flash-002.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_mistralai_Mixtral-8x22B-Instruct-v0.1.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_Qwen_Qwen2.5-72B-Instruct-Turbo.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_claude-3-5-sonnet-20241022.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_claude-3-5-haiku-20241022.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_gpt-4o-2024-08-06.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_gemini-1.5-pro-002.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_meta-llama_Meta-Llama-3.1-70B-Instruct-Turbo.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_meta-llama_Meta-Llama-3.1-8B-Instruct-Turbo.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_deepseek-ai_DeepSeek-V3.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_gemini-2.0-flash-001.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_Qwen_Qwen2.5-7B-Instruct-Turbo.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_deepseek-ai_DeepSeek-R1.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_gemini-2.0-pro-exp-02-05.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_mistralai_Mistral-7B-Instruct-v0.2.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_meta-llama_Meta-Llama-3.1-405B-Instruct-Turbo.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_gemini-1.5-flash-002.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_mistralai_Mixtral-8x22B-Instruct-v0.1.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_Qwen_Qwen2.5-72B-Instruct-Turbo.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_claude-3-5-sonnet-20241022.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_claude-3-5-haiku-20241022.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_gpt-4o-2024-08-06.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_gemini-1.5-pro-002.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_meta-llama_Meta-Llama-3.1-70B-Instruct-Turbo.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_meta-llama_Meta-Llama-3.1-8B-Instruct-Turbo.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_deepseek-ai_DeepSeek-V3.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_gemini-2.0-flash-001.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_gemini-1.5-flash-002_outputs_video_segments.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_gpt-4o-2024-08-06_outputs_video_segments.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_llava-v1.6-mistral-7b-hf_outputs_video_segments.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_gemini-2.0-flash-001_outputs_video_segments.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_gemini-1.5-pro-002_outputs_video_segments.csv\n",
      "Saved cleaned_inference_results/cleaned_segmentwise_gemini-2.0-pro-exp-02-05_outputs_video_segments.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_gemini-1.5-pro-002_outputs_video_full_length.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_gemini-2.0-pro-exp-02-05_outputs_video_full_length.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_gemini-1.5-flash-002_outputs_video_full_length.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_gpt-4o-2024-08-06_outputs_video_full_length.csv\n",
      "Saved cleaned_inference_results/cleaned_whole_gemini-2.0-flash-001_outputs_video_full_length.csv\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the dictionary and save each DataFrame as a CSV\n",
    "for key, dataframe in parsed_inference_dataframes_dictionary.items():\n",
    "    # Clean key to make it a valid filename\n",
    "    sanitized_key = key.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\" \", \"_\")\n",
    "    output_filename = os.path.join(output_dir, f\"cleaned_{sanitized_key}.csv\")\n",
    "    \n",
    "    try:\n",
    "        dataframe.to_csv(output_filename, index=False)\n",
    "        print(f\"Saved {output_filename}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error saving {output_filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conviction score type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: whole_Qwen/Qwen2.5-7B-Instruct-Turbo, Type: <class 'str'>\n",
      "Key: whole_deepseek-ai/DeepSeek-R1, Type: <class 'str'>\n",
      "Key: whole_gemini-2.0-pro-exp-02-05, Type: <class 'str'>\n",
      "Key: whole_mistralai/Mistral-7B-Instruct-v0.2, Type: <class 'str'>\n",
      "Key: whole_meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo, Type: <class 'str'>\n",
      "Key: whole_gemini-1.5-flash-002, Type: <class 'str'>\n",
      "Key: whole_mistralai/Mixtral-8x22B-Instruct-v0.1, Type: <class 'str'>\n",
      "Key: whole_Qwen/Qwen2.5-72B-Instruct-Turbo, Type: <class 'str'>\n",
      "Key: whole_claude-3-5-sonnet-20241022, Type: <class 'str'>\n",
      "Key: whole_claude-3-5-haiku-20241022, Type: <class 'int'>\n",
      "Key: whole_gpt-4o-2024-08-06, Type: <class 'str'>\n",
      "Key: whole_gemini-1.5-pro-002, Type: <class 'str'>\n",
      "Key: whole_meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo, Type: <class 'str'>\n",
      "Key: whole_meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo, Type: <class 'str'>\n",
      "Key: whole_deepseek-ai/DeepSeek-V3, Type: <class 'str'>\n",
      "Key: whole_gemini-2.0-flash-001, Type: <class 'str'>\n",
      "Key: segmentwise_Qwen/Qwen2.5-7B-Instruct-Turbo, Type: <class 'str'>\n",
      "Key: segmentwise_deepseek-ai/DeepSeek-R1, Type: <class 'str'>\n",
      "Key: segmentwise_gemini-2.0-pro-exp-02-05, Type: <class 'str'>\n",
      "Key: segmentwise_mistralai/Mistral-7B-Instruct-v0.2, Type: <class 'str'>\n",
      "Key: segmentwise_meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo, Type: <class 'str'>\n",
      "Key: segmentwise_gemini-1.5-flash-002, Type: <class 'str'>\n",
      "Key: segmentwise_mistralai/Mixtral-8x22B-Instruct-v0.1, Type: <class 'str'>\n",
      "Key: segmentwise_Qwen/Qwen2.5-72B-Instruct-Turbo, Type: <class 'str'>\n",
      "Key: segmentwise_claude-3-5-sonnet-20241022, Type: <class 'str'>\n",
      "Key: segmentwise_claude-3-5-haiku-20241022, Type: <class 'int'>\n",
      "Key: segmentwise_gpt-4o-2024-08-06, Type: <class 'str'>\n",
      "Key: segmentwise_gemini-1.5-pro-002, Type: <class 'str'>\n",
      "Key: segmentwise_meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo, Type: <class 'str'>\n",
      "Key: segmentwise_meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo, Type: <class 'str'>\n",
      "Key: segmentwise_deepseek-ai/DeepSeek-V3, Type: <class 'str'>\n",
      "Key: segmentwise_gemini-2.0-flash-001, Type: <class 'str'>\n",
      "Key: segmentwise_gemini-1.5-flash-002_outputs_video_segments, Type: <class 'str'>\n",
      "Key: segmentwise_gpt-4o-2024-08-06_outputs_video_segments, Type: <class 'str'>\n",
      "Key: segmentwise_llava-v1.6-mistral-7b-hf_outputs_video_segments, Type: <class 'str'>\n",
      "Key: segmentwise_gemini-2.0-flash-001_outputs_video_segments, Type: <class 'str'>\n",
      "Key: segmentwise_gemini-1.5-pro-002_outputs_video_segments, Type: <class 'str'>\n",
      "Key: segmentwise_gemini-2.0-pro-exp-02-05_outputs_video_segments, Type: <class 'str'>\n",
      "Key: whole_gemini-1.5-pro-002_outputs_video_full_length, Type: <class 'int'>\n",
      "Key: whole_gemini-2.0-pro-exp-02-05_outputs_video_full_length, Type: <class 'str'>\n",
      "Key: whole_gemini-1.5-flash-002_outputs_video_full_length, Type: <class 'str'>\n",
      "Key: whole_gpt-4o-2024-08-06_outputs_video_full_length, Type: <class 'str'>\n",
      "Key: whole_gemini-2.0-flash-001_outputs_video_full_length, Type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Iterate over each value (dataframe) in the dictionary\n",
    "for key, dataframe in parsed_inference_dataframes_dictionary.items():\n",
    "    if not dataframe.empty and 'llm_conviction_score' in dataframe.columns:\n",
    "        # Take the first value of the 'llm_conviction_score' column\n",
    "        first_value = dataframe['llm_conviction_score'].iloc[0]\n",
    "        # Print the variable type of the first value\n",
    "        print(f\"Key: {key}, Type: {type(first_value)}\")\n",
    "    else:\n",
    "        print(f\"Key: {key}, DataFrame is either empty or does not contain 'llm_conviction_score' column.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: whole_Qwen/Qwen2.5-7B-Instruct-Turbo, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: whole_Qwen/Qwen2.5-7B-Instruct-Turbo, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_deepseek-ai/DeepSeek-R1, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: whole_deepseek-ai/DeepSeek-R1, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_gemini-2.0-pro-exp-02-05, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: whole_gemini-2.0-pro-exp-02-05, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_mistralai/Mistral-7B-Instruct-v0.2, Before Change - Type: <class 'str'>, Value: 3\n",
      "For whole_mistralai/Mistral-7B-Instruct-v0.2, imputed -1 for conviction score because ValueError: Unclear\n",
      "\n",
      "Key: whole_mistralai/Mistral-7B-Instruct-v0.2, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: whole_meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_gemini-1.5-flash-002, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: whole_gemini-1.5-flash-002, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: whole_mistralai/Mixtral-8x22B-Instruct-v0.1, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: whole_mistralai/Mixtral-8x22B-Instruct-v0.1, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_Qwen/Qwen2.5-72B-Instruct-Turbo, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: whole_Qwen/Qwen2.5-72B-Instruct-Turbo, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_claude-3-5-sonnet-20241022, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: whole_claude-3-5-sonnet-20241022, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: whole_claude-3-5-haiku-20241022, Before Change - Type: <class 'int'>, Value: 2\n",
      "Key: whole_claude-3-5-haiku-20241022, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: whole_gpt-4o-2024-08-06, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: whole_gpt-4o-2024-08-06, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_gemini-1.5-pro-002, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: whole_gemini-1.5-pro-002, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: whole_meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: whole_meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: whole_meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_deepseek-ai/DeepSeek-V3, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: whole_deepseek-ai/DeepSeek-V3, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_gemini-2.0-flash-001, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: whole_gemini-2.0-flash-001, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: segmentwise_Qwen/Qwen2.5-7B-Instruct-Turbo, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: segmentwise_Qwen/Qwen2.5-7B-Instruct-Turbo, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: segmentwise_deepseek-ai/DeepSeek-R1, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: segmentwise_deepseek-ai/DeepSeek-R1, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: segmentwise_gemini-2.0-pro-exp-02-05, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: segmentwise_gemini-2.0-pro-exp-02-05, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: segmentwise_mistralai/Mistral-7B-Instruct-v0.2, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: segmentwise_mistralai/Mistral-7B-Instruct-v0.2, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: segmentwise_meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: segmentwise_meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: segmentwise_gemini-1.5-flash-002, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: segmentwise_gemini-1.5-flash-002, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: segmentwise_mistralai/Mixtral-8x22B-Instruct-v0.1, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: segmentwise_mistralai/Mixtral-8x22B-Instruct-v0.1, After Change - Type: <class 'numpy.int64'>, Value: 3\n",
      "Key: segmentwise_Qwen/Qwen2.5-72B-Instruct-Turbo, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: segmentwise_Qwen/Qwen2.5-72B-Instruct-Turbo, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: segmentwise_claude-3-5-sonnet-20241022, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: segmentwise_claude-3-5-sonnet-20241022, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: segmentwise_claude-3-5-haiku-20241022, Before Change - Type: <class 'int'>, Value: 3\n",
      "Key: segmentwise_claude-3-5-haiku-20241022, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: segmentwise_gpt-4o-2024-08-06, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: segmentwise_gpt-4o-2024-08-06, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: segmentwise_gemini-1.5-pro-002, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: segmentwise_gemini-1.5-pro-002, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: segmentwise_meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: segmentwise_meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: segmentwise_meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: segmentwise_meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: segmentwise_deepseek-ai/DeepSeek-V3, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: segmentwise_deepseek-ai/DeepSeek-V3, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: segmentwise_gemini-2.0-flash-001, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: segmentwise_gemini-2.0-flash-001, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: segmentwise_gemini-1.5-flash-002_outputs_video_segments, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: segmentwise_gemini-1.5-flash-002_outputs_video_segments, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: segmentwise_gpt-4o-2024-08-06_outputs_video_segments, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: segmentwise_gpt-4o-2024-08-06_outputs_video_segments, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: segmentwise_llava-v1.6-mistral-7b-hf_outputs_video_segments, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: segmentwise_llava-v1.6-mistral-7b-hf_outputs_video_segments, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: segmentwise_gemini-2.0-flash-001_outputs_video_segments, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: segmentwise_gemini-2.0-flash-001_outputs_video_segments, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: segmentwise_gemini-1.5-pro-002_outputs_video_segments, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: segmentwise_gemini-1.5-pro-002_outputs_video_segments, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: segmentwise_gemini-2.0-pro-exp-02-05_outputs_video_segments, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: segmentwise_gemini-2.0-pro-exp-02-05_outputs_video_segments, After Change - Type: <class 'numpy.int64'>, Value: 3\n",
      "Key: whole_gemini-1.5-pro-002_outputs_video_full_length, Before Change - Type: <class 'int'>, Value: 3\n",
      "Key: whole_gemini-1.5-pro-002_outputs_video_full_length, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_gemini-2.0-pro-exp-02-05_outputs_video_full_length, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: whole_gemini-2.0-pro-exp-02-05_outputs_video_full_length, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_gemini-1.5-flash-002_outputs_video_full_length, Before Change - Type: <class 'str'>, Value: 2\n",
      "Key: whole_gemini-1.5-flash-002_outputs_video_full_length, After Change - Type: <class 'numpy.float64'>, Value: 2.0\n",
      "Key: whole_gpt-4o-2024-08-06_outputs_video_full_length, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: whole_gpt-4o-2024-08-06_outputs_video_full_length, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Key: whole_gemini-2.0-flash-001_outputs_video_full_length, Before Change - Type: <class 'str'>, Value: 3\n",
      "Key: whole_gemini-2.0-flash-001_outputs_video_full_length, After Change - Type: <class 'numpy.float64'>, Value: 3.0\n",
      "Log file saved at: logs/conviction_score_log.txt\n"
     ]
    }
   ],
   "source": [
    "# Initialize log string\n",
    "log_string = \"\"\n",
    "\n",
    "# Iterate over each value (dataframe) in the dictionary\n",
    "for key, dataframe in parsed_inference_dataframes_dictionary.items():\n",
    "    if not dataframe.empty and 'llm_conviction_score' in dataframe.columns:\n",
    "        # Take the first value of the 'llm_conviction_score' column\n",
    "        first_value = dataframe['llm_conviction_score'].iloc[0]\n",
    "        # Print the variable type of the first value before changing\n",
    "        print(f\"Key: {key}, Before Change - Type: {type(first_value)}, Value: {first_value}\")\n",
    "        \n",
    "        # Apply conversion logic\n",
    "        def convert_value(x):\n",
    "            global log_string\n",
    "            if isinstance(x, str):\n",
    "                try:\n",
    "                    return int(x)  # Try converting to integer\n",
    "                except ValueError:\n",
    "                    current_error = f\"For {key}, imputed -1 for conviction score because ValueError: {x}\\n\"\n",
    "                    print(current_error)\n",
    "                    log_string = log_string + current_error\n",
    "                    return -1  # Assign -1 if conversion fails\n",
    "            return x  # Keep the value as is if not a string\n",
    "        \n",
    "        dataframe['llm_conviction_score'] = dataframe['llm_conviction_score'].apply(convert_value)\n",
    "        first_value = dataframe['llm_conviction_score'].iloc[0]  # Update after conversion\n",
    "        \n",
    "        # Print the variable type of the first value after changing\n",
    "        print(f\"Key: {key}, After Change - Type: {type(first_value)}, Value: {first_value}\")\n",
    "    else:\n",
    "        print(f\"Key: {key}, DataFrame is either empty or does not contain 'llm_conviction_score' column.\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)  # Creates the directory if it doesn't exist\n",
    "\n",
    "\n",
    "# Output the log string to a file\n",
    "log_file_path = \"logs/conviction_score_log.txt\"\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    log_file.write(log_string)\n",
    "\n",
    "print(f\"Log file saved at: {log_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting completed - ready to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dataset = pd.read_csv(\"../data_needed_for_inference/complete_dataset_with_refined_prices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single F1: 0.5\n",
      "Pairs F1: 1.0\n",
      "Triplets F1: 0.5\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def f1_score_calculator_all(ground_truth, predicted, tuple_type=\"single\"):\n",
    "    \"\"\"\n",
    "    Calculate F1 score for:\n",
    "      - single:   list of strings\n",
    "      - pairs:    list of 2-tuples of (string, string)\n",
    "      - triplets: list of 3-tuples of (string, string, numeric)\n",
    "    \n",
    "    :param ground_truth: list or string representation of list\n",
    "    :param predicted: list or string representation of list\n",
    "    :param tuple_type: one of {\"single\", \"pairs\", \"triplets\"}\n",
    "    :return: F1 score (float)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Parse inputs if provided as strings\n",
    "    if isinstance(ground_truth, str):\n",
    "        ground_truth = ast.literal_eval(ground_truth)\n",
    "    if isinstance(predicted, str):\n",
    "        predicted = ast.literal_eval(predicted)\n",
    "\n",
    "    # 2. Validate and normalize ground_truth/predicted depending on tuple_type\n",
    "    if tuple_type == \"single\":\n",
    "        # Expect: list of strings, e.g. [\"AAPL\", \"META\"]\n",
    "        def is_valid_single_list(data_list):\n",
    "            return (isinstance(data_list, list) and \n",
    "                    all(isinstance(item, str) for item in data_list))\n",
    "\n",
    "        if not (is_valid_single_list(ground_truth) and \n",
    "                is_valid_single_list(predicted)):\n",
    "            return 0.0  # Invalid format\n",
    "\n",
    "        # Normalize: convert each string to lowercase\n",
    "        ground_truth_set = set(item.lower() for item in ground_truth)\n",
    "        predicted_set    = set(item.lower() for item in predicted)\n",
    "\n",
    "    elif tuple_type == \"pairs\":\n",
    "        # Expect: list of 2-tuples (string, string)\n",
    "        def is_valid_pairs_list(data_list):\n",
    "            if not isinstance(data_list, list):\n",
    "                return False\n",
    "            for tup in data_list:\n",
    "                if not (isinstance(tup, tuple) and len(tup) == 2):\n",
    "                    return False\n",
    "                if not all(isinstance(x, str) for x in tup):\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        if not (is_valid_pairs_list(ground_truth) and\n",
    "                is_valid_pairs_list(predicted)):\n",
    "            return 0.0  # Invalid format\n",
    "\n",
    "        # Normalize: (str.lower(), str.lower())\n",
    "        def normalize_pair(tup):\n",
    "            return (tup[0].lower(), tup[1].lower())\n",
    "        \n",
    "        ground_truth_set = set(normalize_pair(tup) for tup in ground_truth)\n",
    "        predicted_set    = set(normalize_pair(tup) for tup in predicted)\n",
    "\n",
    "    elif tuple_type == \"triplets\":\n",
    "        # Expect: list of 3-tuples (string, string, numeric)\n",
    "        def is_valid_triplets_list(data_list):\n",
    "            if not isinstance(data_list, list):\n",
    "                return False\n",
    "            for tup in data_list:\n",
    "                if not (isinstance(tup, tuple) and len(tup) == 3):\n",
    "                    return False\n",
    "                # First two are strings, last one is numeric\n",
    "                if not (isinstance(tup[0], str) and \n",
    "                        isinstance(tup[1], str) and \n",
    "                        isinstance(tup[2], (int, float))):\n",
    "                    return False\n",
    "            return True\n",
    "        \n",
    "        if not (is_valid_triplets_list(ground_truth) and\n",
    "                is_valid_triplets_list(predicted)):\n",
    "            return 0.0  # Invalid format\n",
    "\n",
    "        # Normalize: lower the first two strings, keep numeric as-is\n",
    "        def normalize_triplet(tup):\n",
    "            return (tup[0].lower(), tup[1].lower(), tup[2])\n",
    "\n",
    "        ground_truth_set = set(normalize_triplet(tup) for tup in ground_truth)\n",
    "        predicted_set    = set(normalize_triplet(tup) for tup in predicted)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported tuple_type. Choose from 'single', 'pairs', 'triplets'.\")\n",
    "\n",
    "    # 3. Calculate precision, recall, and F1\n",
    "    true_positives = ground_truth_set & predicted_set\n",
    "    false_positives = predicted_set - ground_truth_set\n",
    "    false_negatives = ground_truth_set - predicted_set\n",
    "\n",
    "    precision = len(true_positives) / len(predicted_set) if predicted_set else 0.0\n",
    "    recall    = len(true_positives) / len(ground_truth_set) if ground_truth_set else 0.0\n",
    "    f1_score  = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXAMPLE USAGE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# 1) SINGLE\n",
    "ground_truth_single = [\"AAPL\", \"MTA\"]\n",
    "predicted_single = [\"AAPL\", \"META\"]  # case order is mixed on purpose\n",
    "f1_single = f1_score_calculator_all(ground_truth_single, predicted_single, tuple_type=\"single\")\n",
    "print(f\"Single F1: {f1_single}\")\n",
    "\n",
    "# 2) PAIRS\n",
    "ground_truth_pairs = [(\"AAPL\", \"BUY\"), (\"MSFT\", \"SELL\")]\n",
    "predicted_pairs    = [(\"AAPL\", \"BUY\"), (\"MSFT\", \"SELL\")]\n",
    "f1_pairs = f1_score_calculator_all(ground_truth_pairs, predicted_pairs, tuple_type=\"pairs\")\n",
    "print(f\"Pairs F1: {f1_pairs}\")\n",
    "\n",
    "# 3) TRIPLETS\n",
    "ground_truth_triplets = [(\"AAPL\", \"BUY\", 2), (\"MSFT\", \"SELL\", 3.0)]\n",
    "predicted_triplets    = [(\"aapl\", \"buy\", 2), (\"msft\", \"sell\", 2)]\n",
    "f1_triplets = f1_score_calculator_all(ground_truth_triplets, predicted_triplets, tuple_type=\"triplets\")\n",
    "print(f\"Triplets F1: {f1_triplets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 benchmark: `(ticker_name)` single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm_ticker_name(human_dataset, df_llm_annotations):\n",
    "    # Condense LLM annotations by 'video_id' to a list of ticker names\n",
    "    df_llm_condensed = (\n",
    "        df_llm_annotations\n",
    "        .groupby('video_id', group_keys=False)['llm_ticker_name']\n",
    "        .apply(list)\n",
    "        .reset_index(name='llm_ticker_names')\n",
    "    )\n",
    "\n",
    "    # Condense human annotations by 'video_id' to a list of ticker names\n",
    "    df_human_condensed = (\n",
    "        human_dataset\n",
    "        .groupby('video_id', group_keys=False)['ticker_name']\n",
    "        .apply(list)\n",
    "        .reset_index(name='human_ticker_names')\n",
    "    )\n",
    "\n",
    "    # Merging the dataframes by 'video_id'\n",
    "    df_condensed = pd.merge(df_llm_condensed, df_human_condensed, on='video_id', suffixes=('_llm', '_human'))\n",
    "\n",
    "    # Add the f1_score column to df_condensed\n",
    "    df_condensed['f1_score'] = df_condensed.apply(\n",
    "        lambda row: f1_score_calculator_all(row['human_ticker_names'], row['llm_ticker_names'], \"single\"), axis=1\n",
    "    )\n",
    "\n",
    "    # Calculate Macro F1 Score\n",
    "    macro_f1 = df_condensed['f1_score'].mean()\n",
    "\n",
    "    return macro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5605905207580932"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_llm_ticker_name(human_dataset, parsed_inference_dataframes_dictionary[first_key_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 benchmark: `(ticker_name, action)` pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm_ticker_name_action(human_dataset, df_llm_annotations):\n",
    "    df_llm_condensed = (\n",
    "        df_llm_annotations\n",
    "        .groupby('video_id', group_keys=False)[['llm_ticker_name', 'llm_action']]\n",
    "        .apply(lambda x: list(zip(x['llm_ticker_name'], x['llm_action'])))\n",
    "        .reset_index(name='llm_pairs')\n",
    "    )\n",
    "\n",
    "    df_human_condensed = (\n",
    "        human_dataset\n",
    "        .groupby('video_id', group_keys=False)[['ticker_name', 'action']]\n",
    "        .apply(lambda x: list(zip(x['ticker_name'], x['action'])))\n",
    "        .reset_index(name='human_pairs')\n",
    "    )\n",
    "\n",
    "    # Merging the dataframes by 'video_id'\n",
    "    df_condensed = pd.merge(df_llm_condensed, df_human_condensed, on='video_id', suffixes=('_llm', '_human'))\n",
    "\n",
    "    # Add the f1_score column to df_condensed\n",
    "    df_condensed['f1_score'] = df_condensed.apply(\n",
    "        lambda row: f1_score_calculator_all(row['human_pairs'], row['llm_pairs'], tuple_type=\"pairs\"), axis=1\n",
    "    )\n",
    "\n",
    "    # Calculate Macro F1 Score\n",
    "    macro_f1 = df_condensed['f1_score'].mean()\n",
    "    \n",
    "    return macro_f1\n",
    "\n",
    "    #return df_condensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41011549322714463"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_llm_ticker_name_action(human_dataset, parsed_inference_dataframes_dictionary[first_key_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L3 benchmark: `(ticker_name, action, conviction_score)` triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm_action_ticker_name_conviction_score(human_dataset, df_llm_annotations):\n",
    "    # Creating the LLM triplets dataframe\n",
    "    df_llm_condensed = (\n",
    "        df_llm_annotations\n",
    "        .groupby('video_id', group_keys=False)[['llm_ticker_name', 'llm_action', 'llm_conviction_score']]\n",
    "        .apply(lambda x: list(zip(x['llm_ticker_name'], x['llm_action'], x['llm_conviction_score'])))\n",
    "        .reset_index(name='llm_triplets')\n",
    "    )\n",
    "\n",
    "    # Creating the human triplets dataframe\n",
    "    df_human_condensed = (\n",
    "        human_dataset\n",
    "        .groupby('video_id', group_keys=False)[['ticker_name', 'action', 'conviction_score']]\n",
    "        .apply(lambda x: list(zip(x['ticker_name'], x['action'], x['conviction_score'])))\n",
    "        .reset_index(name='human_triplets')\n",
    "    )\n",
    "\n",
    "    # Merging the dataframes by 'video_id'\n",
    "    df_condensed = pd.merge(df_llm_condensed, df_human_condensed, on='video_id', suffixes=('_llm', '_human'))\n",
    "\n",
    "\n",
    "    # Add the f1_score column to df_condensed\n",
    "    df_condensed['f1_score'] = df_condensed.apply(\n",
    "        lambda row: f1_score_calculator_all(row['human_triplets'], row['llm_triplets'], tuple_type=\"triplets\"), axis=1\n",
    "    )\n",
    "        \n",
    "    # Calculate Macro F1 Score\n",
    "    macro_f1 = df_condensed['f1_score'].mean()\n",
    "    \n",
    "    return macro_f1\n",
    "\n",
    "    #return df_condensed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19540343769820315"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = evaluate_llm_action_ticker_name_conviction_score(human_dataset, parsed_inference_dataframes_dictionary[first_key_name])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE score: number of recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_number_of_recs(human_dataset, df_llm_annotations):\n",
    "    df_llm_condensed = df_llm_annotations.groupby('video_id').size().reset_index(name='llm_recommendation_count')\n",
    "    df_human_condensed = human_dataset.groupby('video_id').size().reset_index(name='human_recommendation_count')\n",
    "\n",
    "\n",
    "    # Merging the dataframes by 'video_id'\n",
    "    df_condensed = pd.merge(df_llm_condensed, df_human_condensed, on='video_id', suffixes=('_llm', '_human'))\n",
    "\n",
    "    # RMSE \n",
    "\n",
    "    mae = np.mean(np.abs(df_condensed['llm_recommendation_count'] - df_condensed['human_recommendation_count']))\n",
    "\n",
    "    return mae\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6840277777777778"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_number_of_recs(human_dataset, parsed_inference_dataframes_dictionary[first_key_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a results dataframe\n",
    "results = []\n",
    "\n",
    "# Iterate through models and their respective dataframes\n",
    "for model_name, df_llm_annotations in parsed_inference_dataframes_dictionary.items():\n",
    "    f1_single = round(evaluate_llm_ticker_name(human_dataset, df_llm_annotations) * 100, 2)\n",
    "    f1_pairs = round(evaluate_llm_ticker_name_action(human_dataset, df_llm_annotations) * 100, 2)\n",
    "    f1_triplets = round(evaluate_llm_action_ticker_name_conviction_score(human_dataset, df_llm_annotations) * 100, 2)\n",
    "    mae_count = evaluate_number_of_recs(human_dataset, df_llm_annotations)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"T\": f1_single,\n",
    "        \"TA\": f1_pairs,\n",
    "        \"TAC\": f1_triplets,\n",
    "        \"Count (MAE)\" : mae_count\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_waterfall_results = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_waterfall_results[\"Avg\"] = df_waterfall_results[[\"Action\", \"Action and ticker\", \"Action, ticker, conviction\"]].mean(axis=1)\n",
    "df_waterfall_results.sort_values(by=[\"TAC\"], ascending=False, inplace=True)\n",
    "#df_waterfall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_waterfall_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Split into 'prefix' and 'base_name'\n",
    "df[['prefix', 'base_name']] = df['Model'].str.split('_', n=1, expand=True)\n",
    "\n",
    "# 2) Pivot using the newly created columns\n",
    "df_pivot = df.pivot(\n",
    "    index='base_name',\n",
    "    columns='prefix',\n",
    "    values=['T','TA','TAC','Count (MAE)']\n",
    ")\n",
    "\n",
    "# 3) Flatten the MultiIndex column names\n",
    "df_pivot.columns = [f\"{col[1]}_{col[0]}\" for col in df_pivot.columns]\n",
    "\n",
    "# 4) Optional: reset index to make 'base_name' a normal column\n",
    "df_pivot = df_pivot.reset_index()\n",
    "\n",
    "# 5) Reorder columns in the desired sequence\n",
    "desired_order = [\n",
    "    'base_name',\n",
    "    'whole_T', 'whole_TA', 'whole_TAC', 'whole_Count (MAE)',\n",
    "    'segmentwise_T', 'segmentwise_TA', 'segmentwise_TAC', 'segmentwise_Count (MAE)'\n",
    "]\n",
    "df_pivot = df_pivot[desired_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot.drop(['whole_Count (MAE)', 'segmentwise_Count (MAE)'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_name</th>\n",
       "      <th>whole_T</th>\n",
       "      <th>whole_TA</th>\n",
       "      <th>whole_TAC</th>\n",
       "      <th>segmentwise_T</th>\n",
       "      <th>segmentwise_TA</th>\n",
       "      <th>segmentwise_TAC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen/Qwen2.5-72B-Instruct-Turbo</td>\n",
       "      <td>65.65</td>\n",
       "      <td>47.42</td>\n",
       "      <td>19.65</td>\n",
       "      <td>79.20</td>\n",
       "      <td>48.36</td>\n",
       "      <td>22.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen/Qwen2.5-7B-Instruct-Turbo</td>\n",
       "      <td>56.06</td>\n",
       "      <td>41.01</td>\n",
       "      <td>19.54</td>\n",
       "      <td>65.57</td>\n",
       "      <td>38.16</td>\n",
       "      <td>20.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claude-3-5-haiku-20241022</td>\n",
       "      <td>56.94</td>\n",
       "      <td>40.66</td>\n",
       "      <td>19.75</td>\n",
       "      <td>68.18</td>\n",
       "      <td>43.61</td>\n",
       "      <td>21.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claude-3-5-sonnet-20241022</td>\n",
       "      <td>65.32</td>\n",
       "      <td>43.38</td>\n",
       "      <td>22.54</td>\n",
       "      <td>75.90</td>\n",
       "      <td>46.47</td>\n",
       "      <td>24.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek-ai/DeepSeek-R1</td>\n",
       "      <td>63.89</td>\n",
       "      <td>45.81</td>\n",
       "      <td>21.29</td>\n",
       "      <td>71.18</td>\n",
       "      <td>47.29</td>\n",
       "      <td>21.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deepseek-ai/DeepSeek-V3</td>\n",
       "      <td>64.91</td>\n",
       "      <td>47.10</td>\n",
       "      <td>23.65</td>\n",
       "      <td>77.56</td>\n",
       "      <td>51.35</td>\n",
       "      <td>28.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gemini-1.5-flash-002</td>\n",
       "      <td>56.90</td>\n",
       "      <td>38.20</td>\n",
       "      <td>18.97</td>\n",
       "      <td>72.09</td>\n",
       "      <td>44.83</td>\n",
       "      <td>21.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gemini-1.5-flash-002_outputs_video_full_length</td>\n",
       "      <td>64.91</td>\n",
       "      <td>45.06</td>\n",
       "      <td>20.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gemini-1.5-flash-002_outputs_video_segments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.23</td>\n",
       "      <td>54.21</td>\n",
       "      <td>23.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gemini-1.5-pro-002</td>\n",
       "      <td>62.78</td>\n",
       "      <td>43.25</td>\n",
       "      <td>21.26</td>\n",
       "      <td>76.56</td>\n",
       "      <td>46.38</td>\n",
       "      <td>22.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gemini-1.5-pro-002_outputs_video_full_length</td>\n",
       "      <td>66.22</td>\n",
       "      <td>46.90</td>\n",
       "      <td>23.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gemini-1.5-pro-002_outputs_video_segments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.01</td>\n",
       "      <td>53.51</td>\n",
       "      <td>24.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gemini-2.0-flash-001</td>\n",
       "      <td>58.36</td>\n",
       "      <td>37.47</td>\n",
       "      <td>19.20</td>\n",
       "      <td>70.05</td>\n",
       "      <td>42.11</td>\n",
       "      <td>21.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gemini-2.0-flash-001_outputs_video_full_length</td>\n",
       "      <td>64.79</td>\n",
       "      <td>40.07</td>\n",
       "      <td>19.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gemini-2.0-flash-001_outputs_video_segments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.24</td>\n",
       "      <td>48.00</td>\n",
       "      <td>23.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gemini-2.0-pro-exp-02-05</td>\n",
       "      <td>60.75</td>\n",
       "      <td>44.59</td>\n",
       "      <td>18.63</td>\n",
       "      <td>74.66</td>\n",
       "      <td>47.18</td>\n",
       "      <td>24.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gemini-2.0-pro-exp-02-05_outputs_video_full_le...</td>\n",
       "      <td>66.42</td>\n",
       "      <td>46.17</td>\n",
       "      <td>18.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gemini-2.0-pro-exp-02-05_outputs_video_segments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.04</td>\n",
       "      <td>54.28</td>\n",
       "      <td>25.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>57.43</td>\n",
       "      <td>37.64</td>\n",
       "      <td>15.35</td>\n",
       "      <td>76.76</td>\n",
       "      <td>45.90</td>\n",
       "      <td>24.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpt-4o-2024-08-06_outputs_video_full_length</td>\n",
       "      <td>64.95</td>\n",
       "      <td>44.53</td>\n",
       "      <td>19.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gpt-4o-2024-08-06_outputs_video_segments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.47</td>\n",
       "      <td>51.15</td>\n",
       "      <td>27.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>llava-v1.6-mistral-7b-hf_outputs_video_segments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.46</td>\n",
       "      <td>11.45</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo</td>\n",
       "      <td>62.83</td>\n",
       "      <td>45.82</td>\n",
       "      <td>22.67</td>\n",
       "      <td>78.60</td>\n",
       "      <td>49.50</td>\n",
       "      <td>25.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo</td>\n",
       "      <td>63.37</td>\n",
       "      <td>45.51</td>\n",
       "      <td>19.29</td>\n",
       "      <td>76.51</td>\n",
       "      <td>50.64</td>\n",
       "      <td>23.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo</td>\n",
       "      <td>57.80</td>\n",
       "      <td>40.69</td>\n",
       "      <td>19.08</td>\n",
       "      <td>71.86</td>\n",
       "      <td>43.42</td>\n",
       "      <td>21.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>49.94</td>\n",
       "      <td>34.78</td>\n",
       "      <td>14.13</td>\n",
       "      <td>63.73</td>\n",
       "      <td>39.23</td>\n",
       "      <td>19.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mistralai/Mixtral-8x22B-Instruct-v0.1</td>\n",
       "      <td>57.60</td>\n",
       "      <td>39.86</td>\n",
       "      <td>16.49</td>\n",
       "      <td>70.12</td>\n",
       "      <td>41.75</td>\n",
       "      <td>19.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            base_name  whole_T  whole_TA  \\\n",
       "0                     Qwen/Qwen2.5-72B-Instruct-Turbo    65.65     47.42   \n",
       "1                      Qwen/Qwen2.5-7B-Instruct-Turbo    56.06     41.01   \n",
       "2                           claude-3-5-haiku-20241022    56.94     40.66   \n",
       "3                          claude-3-5-sonnet-20241022    65.32     43.38   \n",
       "4                             deepseek-ai/DeepSeek-R1    63.89     45.81   \n",
       "5                             deepseek-ai/DeepSeek-V3    64.91     47.10   \n",
       "6                                gemini-1.5-flash-002    56.90     38.20   \n",
       "7      gemini-1.5-flash-002_outputs_video_full_length    64.91     45.06   \n",
       "8         gemini-1.5-flash-002_outputs_video_segments      NaN       NaN   \n",
       "9                                  gemini-1.5-pro-002    62.78     43.25   \n",
       "10       gemini-1.5-pro-002_outputs_video_full_length    66.22     46.90   \n",
       "11          gemini-1.5-pro-002_outputs_video_segments      NaN       NaN   \n",
       "12                               gemini-2.0-flash-001    58.36     37.47   \n",
       "13     gemini-2.0-flash-001_outputs_video_full_length    64.79     40.07   \n",
       "14        gemini-2.0-flash-001_outputs_video_segments      NaN       NaN   \n",
       "15                           gemini-2.0-pro-exp-02-05    60.75     44.59   \n",
       "16  gemini-2.0-pro-exp-02-05_outputs_video_full_le...    66.42     46.17   \n",
       "17    gemini-2.0-pro-exp-02-05_outputs_video_segments      NaN       NaN   \n",
       "18                                  gpt-4o-2024-08-06    57.43     37.64   \n",
       "19        gpt-4o-2024-08-06_outputs_video_full_length    64.95     44.53   \n",
       "20           gpt-4o-2024-08-06_outputs_video_segments      NaN       NaN   \n",
       "21    llava-v1.6-mistral-7b-hf_outputs_video_segments      NaN       NaN   \n",
       "22      meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo    62.83     45.82   \n",
       "23       meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo    63.37     45.51   \n",
       "24        meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo    57.80     40.69   \n",
       "25                 mistralai/Mistral-7B-Instruct-v0.2    49.94     34.78   \n",
       "26              mistralai/Mixtral-8x22B-Instruct-v0.1    57.60     39.86   \n",
       "\n",
       "    whole_TAC  segmentwise_T  segmentwise_TA  segmentwise_TAC  \n",
       "0       19.65          79.20           48.36            22.77  \n",
       "1       19.54          65.57           38.16            20.76  \n",
       "2       19.75          68.18           43.61            21.54  \n",
       "3       22.54          75.90           46.47            24.60  \n",
       "4       21.29          71.18           47.29            21.36  \n",
       "5       23.65          77.56           51.35            28.17  \n",
       "6       18.97          72.09           44.83            21.31  \n",
       "7       20.66            NaN             NaN              NaN  \n",
       "8         NaN          86.23           54.21            23.27  \n",
       "9       21.26          76.56           46.38            22.87  \n",
       "10      23.23            NaN             NaN              NaN  \n",
       "11        NaN          86.01           53.51            24.97  \n",
       "12      19.20          70.05           42.11            21.43  \n",
       "13      19.74            NaN             NaN              NaN  \n",
       "14        NaN          82.24           48.00            23.52  \n",
       "15      18.63          74.66           47.18            24.20  \n",
       "16      18.25            NaN             NaN              NaN  \n",
       "17        NaN          86.04           54.28            25.21  \n",
       "18      15.35          76.76           45.90            24.50  \n",
       "19      19.60            NaN             NaN              NaN  \n",
       "20        NaN          83.47           51.15            27.86  \n",
       "21        NaN          16.46           11.45             3.30  \n",
       "22      22.67          78.60           49.50            25.38  \n",
       "23      19.29          76.51           50.64            23.96  \n",
       "24      19.08          71.86           43.42            21.81  \n",
       "25      14.13          63.73           39.23            19.83  \n",
       "26      16.49          70.12           41.75            19.56  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pivot\n",
    "df_pivot.sort_values(by=[\"base_name\"], ascending=True, inplace=True)\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 1: Average across all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall average F1 for T: 67.86185185185185\n",
      "Overall average F1 for TA: 44.25981481481482\n",
      "Overall average F1 for TAC: 21.014074074074074\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Create row-wise average F1 scores for T, TA, TAC\n",
    "df_pivot['T_avg'] = df_pivot[['whole_T', 'segmentwise_T']].mean(axis=1)\n",
    "df_pivot['TA_avg'] = df_pivot[['whole_TA', 'segmentwise_TA']].mean(axis=1)\n",
    "df_pivot['TAC_avg'] = df_pivot[['whole_TAC', 'segmentwise_TAC']].mean(axis=1)\n",
    "\n",
    "# 2. Compute the overall average for each task (across all rows)\n",
    "overall_T_avg = df_pivot['T_avg'].mean()\n",
    "overall_TA_avg = df_pivot['TA_avg'].mean()\n",
    "overall_TAC_avg = df_pivot['TAC_avg'].mean()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nOverall average F1 for T:\", overall_T_avg)\n",
    "print(\"Overall average F1 for TA:\", overall_TA_avg)\n",
    "print(\"Overall average F1 for TAC:\", overall_TAC_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 2: Ticker performance for s and w on current directory selection (models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "66.65 for llm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average whole_T: 61.3247619047619\n",
      "Average segmentwise_T: 73.13545454545455\n",
      "67.23010822510822\n"
     ]
    }
   ],
   "source": [
    "# Compute averages\n",
    "average_whole_T = df_pivot[\"whole_T\"].mean()\n",
    "average_segmentwise_T = df_pivot[\"segmentwise_T\"].mean()\n",
    "\n",
    "# Display results\n",
    "print(f\"Average whole_T: {average_whole_T}\")\n",
    "print(f\"Average segmentwise_T: {average_segmentwise_T}\")\n",
    "print((average_whole_T+average_segmentwise_T)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for key, df in parsed_inference_dataframes_dictionary.items():\\n    if \"llm_action\" in df.columns:\\n        print(f\"Key: {key}\")\\n        print(df[\"llm_action\"].value_counts())\\n        print(\"\\n\" + \"=\"*50 + \"\\n\")\\n    else:\\n        print(f\"Key: {key} - \\'llm_action\\' column not found in dataframe.\\n\")\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for key, df in parsed_inference_dataframes_dictionary.items():\n",
    "    if \"llm_action\" in df.columns:\n",
    "        print(f\"Key: {key}\")\n",
    "        print(df[\"llm_action\"].value_counts())\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    else:\n",
    "        print(f\"Key: {key} - 'llm_action' column not found in dataframe.\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Video IDS being evaluated for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing model: whole_Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "========================================\n",
      "LLM Condensed Unique Count: 288\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 288\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_deepseek-ai/DeepSeek-R1\n",
      "========================================\n",
      "LLM Condensed Unique Count: 279\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 279\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_gemini-2.0-pro-exp-02-05\n",
      "========================================\n",
      "LLM Condensed Unique Count: 188\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 188\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_mistralai/Mistral-7B-Instruct-v0.2\n",
      "========================================\n",
      "LLM Condensed Unique Count: 282\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 282\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\n",
      "========================================\n",
      "LLM Condensed Unique Count: 288\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 288\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_gemini-1.5-flash-002\n",
      "========================================\n",
      "LLM Condensed Unique Count: 288\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 288\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_mistralai/Mixtral-8x22B-Instruct-v0.1\n",
      "========================================\n",
      "LLM Condensed Unique Count: 268\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 268\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_Qwen/Qwen2.5-72B-Instruct-Turbo\n",
      "========================================\n",
      "LLM Condensed Unique Count: 288\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 288\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_claude-3-5-sonnet-20241022\n",
      "========================================\n",
      "LLM Condensed Unique Count: 288\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 288\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_claude-3-5-haiku-20241022\n",
      "========================================\n",
      "LLM Condensed Unique Count: 288\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 288\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_gpt-4o-2024-08-06\n",
      "========================================\n",
      "LLM Condensed Unique Count: 288\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 288\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_gemini-1.5-pro-002\n",
      "========================================\n",
      "LLM Condensed Unique Count: 288\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 288\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n",
      "========================================\n",
      "LLM Condensed Unique Count: 287\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 287\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n",
      "========================================\n",
      "LLM Condensed Unique Count: 281\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 281\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_deepseek-ai/DeepSeek-V3\n",
      "========================================\n",
      "LLM Condensed Unique Count: 268\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 268\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_gemini-2.0-flash-001\n",
      "========================================\n",
      "LLM Condensed Unique Count: 288\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 288\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_deepseek-ai/DeepSeek-R1\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_gemini-2.0-pro-exp-02-05\n",
      "========================================\n",
      "LLM Condensed Unique Count: 141\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 141\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_mistralai/Mistral-7B-Instruct-v0.2\n",
      "========================================\n",
      "LLM Condensed Unique Count: 140\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 140\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_gemini-1.5-flash-002\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_mistralai/Mixtral-8x22B-Instruct-v0.1\n",
      "========================================\n",
      "LLM Condensed Unique Count: 149\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 149\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_Qwen/Qwen2.5-72B-Instruct-Turbo\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_claude-3-5-sonnet-20241022\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_claude-3-5-haiku-20241022\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_gpt-4o-2024-08-06\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_gemini-1.5-pro-002\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n",
      "========================================\n",
      "LLM Condensed Unique Count: 148\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 148\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_deepseek-ai/DeepSeek-V3\n",
      "========================================\n",
      "LLM Condensed Unique Count: 147\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 147\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_gemini-2.0-flash-001\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_gemini-1.5-flash-002_outputs_video_segments\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_gpt-4o-2024-08-06_outputs_video_segments\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_llava-v1.6-mistral-7b-hf_outputs_video_segments\n",
      "========================================\n",
      "LLM Condensed Unique Count: 93\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 93\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_gemini-2.0-flash-001_outputs_video_segments\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_gemini-1.5-pro-002_outputs_video_segments\n",
      "========================================\n",
      "LLM Condensed Unique Count: 148\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 148\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: segmentwise_gemini-2.0-pro-exp-02-05_outputs_video_segments\n",
      "========================================\n",
      "LLM Condensed Unique Count: 150\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 150\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_gemini-1.5-pro-002_outputs_video_full_length\n",
      "========================================\n",
      "LLM Condensed Unique Count: 285\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 284\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_gemini-2.0-pro-exp-02-05_outputs_video_full_length\n",
      "========================================\n",
      "LLM Condensed Unique Count: 288\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 287\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_gemini-1.5-flash-002_outputs_video_full_length\n",
      "========================================\n",
      "LLM Condensed Unique Count: 286\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 285\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_gpt-4o-2024-08-06_outputs_video_full_length\n",
      "========================================\n",
      "LLM Condensed Unique Count: 282\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 281\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing model: whole_gemini-2.0-flash-001_outputs_video_full_length\n",
      "========================================\n",
      "LLM Condensed Unique Count: 285\n",
      "Human Condensed Unique Count: 288\n",
      "Merged Condensed Unique Count: 284\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for model_name, df_llm_annotations in parsed_inference_dataframes_dictionary.items():\n",
    "    print(f\"\\nProcessing model: {model_name}\\n\" + \"=\"*40)\n",
    "    \n",
    "    # Create the LLM condensed DataFrame\n",
    "    df_llm_condensed = (\n",
    "        df_llm_annotations\n",
    "        .groupby('video_id', group_keys=False)[['llm_ticker_name', 'llm_action', 'llm_conviction_score']]\n",
    "        .apply(lambda x: list(zip(x['llm_ticker_name'], x['llm_action'], x['llm_conviction_score'])))\n",
    "        .reset_index(name='llm_triplets')\n",
    "    )\n",
    "    \n",
    "    # Create the human condensed DataFrame\n",
    "    df_human_condensed = (\n",
    "        human_dataset\n",
    "        .groupby('video_id', group_keys=False)[['ticker_name', 'action', 'conviction_score']]\n",
    "        .apply(lambda x: list(zip(x['ticker_name'], x['action'], x['conviction_score'])))\n",
    "        .reset_index(name='human_triplets')\n",
    "    )\n",
    "    \n",
    "    # Merge the two DataFrames on 'video_id'\n",
    "    df_condensed = pd.merge(df_llm_condensed, df_human_condensed, on='video_id', suffixes=('_llm', '_human'))\n",
    "    \n",
    "    # Count unique video IDs in each DataFrame\n",
    "    count_llm = df_llm_condensed['video_id'].nunique()\n",
    "    count_human = df_human_condensed['video_id'].nunique()\n",
    "    count_merged = df_condensed['video_id'].nunique()\n",
    "    \n",
    "    # Print the counts with clear formatting\n",
    "    print(f\"LLM Condensed Unique Count: {count_llm}\")\n",
    "    print(f\"Human Condensed Unique Count: {count_human}\")\n",
    "    print(f\"Merged Condensed Unique Count: {count_merged}\")\n",
    "    print(\"-\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
