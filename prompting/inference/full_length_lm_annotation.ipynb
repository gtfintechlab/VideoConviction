{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is in python 3.10 +. Need to reflect that in environment yaml. \n",
    "\n",
    "output_df.to_csv(\n",
    "        f'../llm_prompt_s_transcript_outputs/{model_name.replace(\"/\", \"_\")}_{today.strftime(\"%d_%m_%Y\")}.csv',\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "Did not run deekseek. deepseek-ai/DeepSeek-R1 Need to have Agam run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep, time\n",
    "from datetime import date\n",
    "import threading\n",
    "from typing import Tuple\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "from google.ai import generativelanguage as glm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "import anthropic\n",
    "today = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure APIs and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't want to use environmental variables you can add your api key directly to api_key\n",
    "togetherai_api_key = os.environ.get('TOGETHERAI_API_KEY')\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "google_api_key = os.environ.get('GOOGLE_API_KEY')\n",
    "anthropic_api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "# together.ai key\n",
    "client_together = OpenAI(api_key=togetherai_api_key,\n",
    "                base_url='https://api.together.xyz')\n",
    "\n",
    "#  OpenAI key\n",
    "client_openai = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Gemini key (https://ai.google.dev/gemini-api/docs/quickstart?lang=python)\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "# Anthropic API Key\n",
    "client_anthropic = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "def get_safety_settings():\n",
    "    \"\"\" \n",
    "    Google API Specific\n",
    "    Set the block threshold to None for each harm category\n",
    "    Refer https://ai.google.dev/gemini-api/docs/safety-settings to modify the safety settings\n",
    "    Gemini model information: https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-pro\n",
    "    \"\"\"\n",
    "    safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    ]\n",
    "    return safety_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not need this line for the segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "#data_directory = '../process_annotations_pipeline/data_sources/'\n",
    "\n",
    "#video_transcripts = pd.read_csv(data_directory + 'full_raw_video_transcripts.csv')\n",
    "#video_metadata = pd.read_csv(data_directory + 'inner_id_to_video_metadata.csv')\n",
    "\n",
    "#video_transcripts.rename(columns = {'transcript_video_id': 'video_id'},\n",
    "#                         inplace = True)\n",
    "\n",
    "# Merging dataframes\n",
    "#df = video_transcripts.merge(\n",
    "#    video_metadata[['video_id', 'original_video_title']],  # Selecting only video_id and original_video_title from video_metadata\n",
    "#    on='video_id',                              # Merging on the video_id column\n",
    "#    how='left'                                 # Keeping all rows from video_transcripts\n",
    "#)\n",
    "\n",
    "# Yash has 288\n",
    "#len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(series, lm_type = \"lm\", whole = True, price = False):\n",
    "    \"\"\"\n",
    "    Generates a structured prompt based on the provided title, transcript, language model type, and whether we.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The pandas series  \n",
    "        lm_type (str, optional): The type of language model the prompt is intended for (default is \"lm\").\n",
    "                                 Options are \"lm\" or \"vlm\". vlm has a slightly larger prompt to incorporate facial expression.\n",
    "        whole (bool, optional): Specifies whether to use the entire transcript (True) or a subset (False).\n",
    "                                Defaults to True.\n",
    "        price (bool, optional): Only available for vlm. Can the vlm detect price.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted prompt stored in the variable `prompt`, ready for input into a language model.\n",
    "    \"\"\"\n",
    "\n",
    "    # title\n",
    "    # transcript\n",
    "    # 'segment_transcript'\n",
    "\n",
    "    title = series['video_title']\n",
    "\n",
    "    if whole == True:\n",
    "        transcript = series['transcript']\n",
    "    if whole == False:   \n",
    "        segment_transcript = series['segment_transcript']\n",
    "    \n",
    "    if lm_type == \"vlm\":\n",
    "        facial_expression = (\"\\n           - Facial Expressions: Neutral or doubtful (furrowed brows, pursed lips).\",\n",
    "                             \"\\n           - Facial Expressions: Moderate enthusiasm (mild smiles, slightly raised eyebrows).\",\n",
    "                             \"\\n           - Facial Expressions: Enthusiastic, energetic (wide smiles, raised eyebrows).\")\n",
    "    else:\n",
    "        facial_expression = (\"\",\n",
    "                             \"\",\n",
    "                             \"\")\n",
    "\n",
    "    if whole == True:\n",
    "        whole_transcript_specific = (\" and video title\",\n",
    "                            f\"\"\"Inputs:\n",
    "    - Video Title: {title}\n",
    "    - Transcript: {transcript}\"\"\",\n",
    "                            \"\\n           - Consistency: Low conviction if the title makes a bold claim, but the transcript lacks matching conviction.\",\n",
    "                            \"\\n           - Consistency: Medium conviction if the title makes a bold claim, followed by consistent confidence in the transcript.\",\n",
    "                            \"\\n           - Consistency: High conviction if the title and transcript are strongly aligned.\")\n",
    "    else:\n",
    "        whole_transcript_specific = (\"\",\n",
    "                            f\"\"\"Inputs:\n",
    "    - Transcript: {segment_transcript}\"\"\",\n",
    "                             \"\",\n",
    "                             \"\",\n",
    "                            \"\")\n",
    "        \n",
    "\n",
    "    prompt = f\"\"\"Analyze the YouTube video transcript{whole_transcript_specific[0]} of influencers discussing the US stock market, focusing on stock recommendations and their conviction.\n",
    "    \n",
    "    {whole_transcript_specific[1]}\n",
    "    \n",
    "    Instructions:\n",
    "    1. Does the video contain any stock recommendations:\n",
    "       - Label this as `Stock Recommendations Present` with either \"Yes\" or \"No\".\n",
    "    \n",
    "    2. If `Stock Recommendations Present` is \"Yes\", create a list under the key `Recommendations`. Each recommendation should follow this structure:{{\"Action\": \"Buy | Hold | Don't Buy | Sell | Short Sell | Unclear\",\n",
    "         \"Justification\": \"Brief explanation for the action based on the transcript\",\n",
    "         \"Conviction Score\": \"1 | 2 | 3\",\n",
    "         \"Ticker Name\": \"Ticker name\"}}\n",
    "    \n",
    "       Details for each field:\n",
    "       - `Action`: Categorize each stock recommendation as:\n",
    "         - \"Buy\": Purchase shares of the stock.\n",
    "         - \"Hold\":  Retain the stock if already owned, without necessarily\n",
    "    buying more.\n",
    "         - \"Don't Buy\": Refrain from purchasing the stock.\n",
    "         - \"Sell\": Sell shares of the stock currently owned.\n",
    "         - \"Short Sell\": Sell shares not currently owned, intending to\n",
    "    buy them back later at a lower price.\n",
    "         - \"Unclear\": When the action is not explicitly stated.\n",
    "       - `Justification`: Provide a brief explanation for the action based on the transcript.\n",
    "       - `Conviction Score`: Assign a score based on the following criteria:\n",
    "         - \"1\" (Low Conviction):\n",
    "           - Tone: Hesitant or uncertain language, frequent qualifiers (e.g., “maybe,” “possibly”).{facial_expression[0]}\n",
    "           - Delivery: Reserved or doubtful language.{whole_transcript_specific[2]}\n",
    "         - \"2\" (Moderate Conviction):\n",
    "           - Tone: Relatively confident language with some qualifiers.{facial_expression[1]}\n",
    "           - Delivery: Balanced and moderately positive language.{whole_transcript_specific[3]}\n",
    "         - \"3\" (High Conviction):\n",
    "           - Tone: Strong, assertive language without hesitation.{facial_expression[2]}\n",
    "           - Delivery: Decisive recommendations with no qualifiers.{whole_transcript_specific[4]}\n",
    "       - `Ticker Name`: Specify the ticker name of the stock being discussed.\n",
    "    \n",
    "    3. If `Stock Recommendations Present` is \"No\", return the following structure:{{\"Stock Recommendations Present\": \"No\",\n",
    "         \"Recommendations\": []\n",
    "       }}\n",
    "    \n",
    "    \n",
    "    Output Requirements:\n",
    "    - Return only valid JSON that can be directly parsed by JSON libraries.\n",
    "    - Do not include any additional text, comments, formatting indicators (e.g., `json` or backticks), or explanatory content.\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_model(prompt: str,\n",
    "                    model_source: str,\n",
    "                    model_name: str,\n",
    "                    temperature: float) -> str:\n",
    "    \"\"\"\n",
    "    Generates a structured prompt based on the provided title, transcript, language model type, and other options.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt.\n",
    "        model_source (str): Source of the model (e.g., 'OpenAI', 'Gemini', or other specified sources).\n",
    "        model_name (str): Name of the model.\n",
    "        temperature (float): Temperature setting for the model.\n",
    "        \n",
    "    Returns:\n",
    "        str: A string formatted as JSON or output from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_json = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        match model_source:\n",
    "            # Try to make Max output tokens\n",
    "            case \"OpenAI\":\n",
    "                model_str = model_name\n",
    "                chat_completion = client_openai.chat.completions.create(\n",
    "                    model=model_str,\n",
    "                    messages=prompt_json,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=2048\n",
    "                )\n",
    "                prompt_output = chat_completion.choices[0].message.content\n",
    "\n",
    "            case \"Gemini\":\n",
    "                model_str = model_name\n",
    "                safety_settings = get_safety_settings()\n",
    "                # Initialize the model once before the iteration over the rows begin.\n",
    "                # Link below takes about how to deal with json markdown output\n",
    "                # https://medium.com/google-cloud/how-to-consistently-output-json-with-the-gemini-api-using-controlled-generation-887220525ae0\n",
    "                model = genai.GenerativeModel(model_name = model_str, \\\n",
    "                                              safety_settings = safety_settings,\n",
    "                                              generation_config=genai.GenerationConfig(\\\n",
    "                                                  max_output_tokens=2048,\\\n",
    "                                                  response_mime_type=\"application/json\",\\\n",
    "                                                    temperature=temperature)\n",
    "                                            )\n",
    "                \n",
    "                response = model.generate_content(prompt)\n",
    "                \n",
    "                if response.text:\n",
    "                    prompt_output = response.text\n",
    "                else:\n",
    "                    print(\"Prompt:\", prompt)\n",
    "                    if response.prompt_feedback:\n",
    "                        print(\"Prompt Feedback: \", response.prompt_feedback)\n",
    "                    if response.candidates[0].finish_reason != glm.Candidate.FinishReason.STOP:\n",
    "                        print(\"Prompt Finish reason: \", response.candidates[0].finish_reason)\n",
    "                        print(\"Prompt Safety Warnings: \", response.candidates[0].safety_ratings)\n",
    "                    prompt_output = \"\"\n",
    "\n",
    "            case \"Anthropic\":\n",
    "                response = client_anthropic.messages.create(\n",
    "                    model=model_name, # Takes parsed parameter in function and uses that model for inference\n",
    "                    temperature = temperature,\n",
    "                    max_tokens=2048,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\", \n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "            \n",
    "                prompt_output = response.content[0].text\n",
    "\n",
    "            case _:\n",
    "                model_str = model_name\n",
    "                chat_completion = client_together.chat.completions.create(\n",
    "                    model=model_str,\n",
    "                    messages=prompt_json,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=2048\n",
    "                )\n",
    "                prompt_output = chat_completion.choices[0].message.content\n",
    "\n",
    "        # Don't inference too fast\n",
    "        sleep(0.4)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sleep(10.0)\n",
    "        prompt_output = \"\"\n",
    "\n",
    "    # Basic cleaning of ```json start\n",
    "    # https://community.openai.com/t/json-returning-with-json/584818/8\n",
    "    def clean_json_string(json_string):\n",
    "        pattern = r'^```json\\s*(.*?)\\s*```$'\n",
    "        cleaned_string = re.sub(pattern, r'\\1', json_string, flags=re.DOTALL)\n",
    "        return cleaned_string.strip()\n",
    "\n",
    "    prompt_output = clean_json_string(prompt_output).strip('```')\n",
    "    \n",
    "    # Remove triple backticks for a couple models that output them \n",
    "    return prompt_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprompt_output = inference_model(prompt = prompt,\\n                model_source =  \"Anthropic\",\\n                model_name = \"claude-3-5-sonnet-20241022\",\\n                temperature = 0)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "prompt_output = inference_model(prompt = prompt,\n",
    "                model_source =  \"Anthropic\",\n",
    "                model_name = \"claude-3-5-sonnet-20241022\",\n",
    "                temperature = 0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(prompt_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Section to Verify Prompt Before Rerunning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"../data_needed_for_inference/w_transcripts_for_inference.csv\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # Extract title and transcript\n",
    "    series = row\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = create_prompt(series = series,\n",
    "              lm_type = 'lm',\n",
    "              whole = True,\n",
    "              price = False\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze the YouTube video transcript and video title of influencers discussing the US stock market, focusing on stock recommendations and their conviction.\n",
      "    \n",
      "    Inputs:\n",
      "    - Video Title: 5 Stocks to Buy Now to Double Your Money\n",
      "    - Transcript:  Hey Bowtie Nation, Joseph Hogue here with the Let's Talk Money channel and finally answering a question I get all the time, what are the stocks to buy that could double fast? And I kind of hate the question but totally understand where it's coming from. Nation, 131 stocks have doubled over the last year with even those mega-cap companies like Apple, Tesla and even NVIDIA finding new highs. And our little Bowtie Nation has benefited from it just as much as anyone with recommendations like Zscaler up 151%, Teladoc up 168% and shares of Fastly jumping by 197%. The problem is, investors think this is normal. Nation, the average annual return on the stock market over the last 30 years is 11%. And that's not bad. An 11% annual return turns a single $1,000 investment into $23,000 over three decades of investing and just $200 a month grows to over half a million dollars. It can make you rich but it takes seven years to double your money at that rate. Are there ways to find stocks to buy that can double or even triple your money? Absolutely, and I'm going to show you how to do that in this video but I don't want you to get those dollar signs in your eyes and expect to double your money every year. In this video, I'll show you the three investing rules to finding stocks that could double, a three-step process to finding those moonshot returns. I'll then reveal five stocks with the potential for triple-digit returns over the next year and beyond. In conclusion, this is exactly the kind of thing I did as a venture capital analyst and using this process, you can consistently produce annual returns of 27% or more on your portfolio. It's this kind of thing I love talking about here on the channel so if you're not part of the community yet, just click that little red subscribe button. It's free and you'll never miss an episode. I'm excited to get into the list of stocks to buy so let's get started and I'll show you the three-step process to finding these in a minute. First up is $225 million dollar Veritone Inc., ticker VERI, a cloud-based AI platform that structures audio and video data. And now Nation, if that sounds like a bunch of tech jargon, just understand that Veritone is in the convergence of what will be the three biggest trends over the next decade. Data analysis of audio and video content, cloud-based connectivity, and an AI platform that learns to become more effective. And the potential markets for these speak for themselves. The company's AIware platform is facing markets with double-digit annual growth and tens of billions in opportunities. Revenue has jumped 244% in the last two years and was even able to increase in that tough second quarter. Analysts have price targets ranging from $15 a share to as high as $18 each over the next year for a potential 120% return and I think this one goes well beyond that long-term. I'll be putting these stocks first in my paper portfolio on the Webull app and then into my real portfolio on the platform. I love this stock simulator here. Webull gives you a million dollars to test out your strategies and stock ideas right in the stock simulator. I can add the stocks to my portfolio, get all the charting and news I need to follow the investment in and track it until I know I want to commit and invest real money. It's a great feature on the app and will help make sure you're only investing in the best stocks. I'll leave a link to the Webull app in the video description below. Click through and not only will you get the stock simulator but Webull is also going to give you a free share of stock worth up to $1,400 when you sign up and make your first deposit. Zynex, ticker ZYXI, is a stock I recommended last October as one of my favorite picks under $10 and it immediately jumped to over $24 a share. It's come back down a little to $16 a share but there is still a lot of potential here. This company has been around for 23 years but just graduated to listing on the Nasdaq this year which is a big boost to that credibility and investor sentiment. Zynex earns most of its revenue from a non-invasive electrotherapy pain management device. That's about 90% of the sales with 60% of it recurring on a monthly supplies basis. I love that recurring chunk of revenue and the company is developing a blood volume monitor and growing into the EEG diagnostic space. Zynex has booked 12 consecutive profitable quarters and orders were up 65% in the first half of 2019. It also paid out a special dividend in the fourth quarter of last year which is extremely rare for small cap stocks to pay out a dividend but the company has $10 million in balance sheet cash and no long-term debt so an excellent financial position. Analysts have price targets from $22.50 per share to as high as $30 each over the next year for a potential 82% return. Now I want to share with you the three-step process for finding these kinds of high return stocks because as well as I think these five stocks can do, you have to be able to pick your own stocks to buy. I want you to be a better investor, I want you to be able to make your own decisions. And first here is you have to think like a venture capitalist, like an early-stage investor. This was my job as an analyst, to find the best startups with the most potential. Like I said, this year has been absolutely crazy. You've had trillion-dollar companies doubling in value with shares of Apple, Amazon, really all those heavyweight tech names surging. But that rarely happens and in fact, the numbers are stacked up against them. For shares of Apple to double again, the company would need to book more than half a trillion dollars in revenue annually and the market cap would be nearly a sixth the entire U.S. economy. But that kind of return is absolutely possible in these small startup companies. This chart is from a survey of angel investors, over 3,000 investments in startup companies, so exactly what we're talking about here and this is showing you the percentage return in these types of investments. Look at the three boxes on the right, 16% of the investments, so around 2 in 10, return five to more than 30 times your money and the second bar here, more than 3 in 10, return up to 500%, five times your money. So applying this to investing in stocks, you're not looking for the most popular stocks or the mega-cap companies. You're looking at the smallest companies, those neglected startups with great futures. To do this, I'll usually start with a screener for stocks under a billion-dollar market cap, that's the value of all the shares outstanding. I'm also looking for companies that are relatively new to the market. I don't want a company that's been around for decades and has never done anything. Second here, it's all about growth and how big the company can become. I'm looking for companies with a giant addressable market and proven sales growth over the last few years. Generally, this is going to mean sales growth of at least 20% or more over the last couple of years so that's easy enough to screen for when you're looking for these stocks to buy. A large addressable market, that's the size of the potential customer base and management's estimate for what kind of market share they can take in the future. This information is usually found in the company's financial reports or in presentations on its investor relations page. For example, researching Fastly, I found it was estimating a $35 billion market for its two segments and a compelling case for taking market share that's turned into a nearly 200% return on the shares. I've got to warn all you value investors out there, even these types of undiscovered stocks are going to have ridiculously high valuation multiples. For example, recommending Fastly last year, earnings weren't even positive yet and the shares were still trading at 10-times sales. That doesn't mean value investors can't get in on the action here, just understand you're going to have to adjust what you think of as value on the P-E and sales multiples. Third here is going to take some research because you've got to find those companies with a competitive advantage. Understand these companies aren't competing in an empty market. These startups are coming into industries with established players and they need a compelling reason why customers should switch to them. For example, with Zscaler, I saw a unique cloud-based security product that could take market share from some of those legacy providers that were still dependent on the hardware and data center product. So it's not as simple as picking the fastest growing or most popular stocks to buy but with just a little research here, you can find stocks that will make you rich. Our next stock, $500 million Midic Systems, ticker MITK, is a leader in mobile capture and identity verification with over 7,500 customers mostly in the financial services sector. There is a huge market for identity verification, especially on that shift to mobile, with the company estimating a $12.7 billion market by 2024 touching nearly every sector. The company has grown revenue at a 32% annualized pace since 2012 and I think it's just scratching the surface here. Midic is booking $85 million in revenue on a market potential in the tens of billions. Once this software reaches that critical mass of customers though, I think this thing could move into more sectors beyond just financials and blow up. Surprisingly, analysts only have targets around the current share price, from $10 per share to as high as $12.50 each but I think this one could be a breakout story over the next few years. Our next pick here is a healthcare stock, $340 million Kourou Medical Systems, ticker KRMD. Kourou produces a unique syringe infusion system that patients can use at home to give themselves injections under the skin without having to inject directly into the vein. The U.S. market alone here is nearly half a billion dollars in annual sales and globally it could be two or three times that number. The company grew sales by 44% in the second quarter and has booked a 37% annual sales growth over the last two years. Beyond that growth though, Kourou has a pristine balance sheet with over $38 million in cash reserves against just $3.8 million in total debt. Analysts have targets from $13 per share to as high as $16 each over the next year so even the lowest analyst target is for a return of 67% from here. Our next stock to buy here, Iradomid, ticker IRMD, produces the world's only non-magnetic portable MRI systems. Iradomid has 17 patents and four pending to help solve the trend in MRI adverse events caused by magnetic interference and other safety problems. The company believes it's a $3.1 billion market across three core products and sales have increased at a 29% annual pace over the two years to 2019 with 80% of revenue from the U.S. and over 2.5 million patients. This is another one with a great balance sheet to help drive that growth. The company has over $48 million in balance sheet cash against just $2.8 million in debt so almost 17% of the market value of this stock is in cash reserves. Last one, Analyst Target here with an estimate of $26 per share but Yahoo Finance shows an average target of $28 per share from several other analysts so potentially a 27% return or more. Click on the video to the right to see how I test my stock trading ideas without risking my own money, how to use a stock simulator to test out your trades. Don't forget to join the Let's Talk Money community by tapping that subscribe button and clicking the bell notification.\n",
      "    \n",
      "    Instructions:\n",
      "    1. Does the video contain any stock recommendations:\n",
      "       - Label this as `Stock Recommendations Present` with either \"Yes\" or \"No\".\n",
      "    \n",
      "    2. If `Stock Recommendations Present` is \"Yes\", create a list under the key `Recommendations`. Each recommendation should follow this structure:{\"Action\": \"Buy | Hold | Don't Buy | Sell | Short Sell | Unclear\",\n",
      "         \"Justification\": \"Brief explanation for the action based on the transcript\",\n",
      "         \"Conviction Score\": \"1 | 2 | 3\",\n",
      "         \"Ticker Name\": \"Ticker name\"}\n",
      "    \n",
      "       Details for each field:\n",
      "       - `Action`: Categorize each stock recommendation as:\n",
      "         - \"Buy\": Purchase shares of the stock.\n",
      "         - \"Hold\":  Retain the stock if already owned, without necessarily\n",
      "    buying more.\n",
      "         - \"Don't Buy\": Refrain from purchasing the stock.\n",
      "         - \"Sell\": Sell shares of the stock currently owned.\n",
      "         - \"Short Sell\": Sell shares not currently owned, intending to\n",
      "    buy them back later at a lower price.\n",
      "         - \"Unclear\": When the action is not explicitly stated.\n",
      "       - `Justification`: Provide a brief explanation for the action based on the transcript.\n",
      "       - `Conviction Score`: Assign a score based on the following criteria:\n",
      "         - \"1\" (Low Conviction):\n",
      "           - Tone: Hesitant or uncertain language, frequent qualifiers (e.g., “maybe,” “possibly”).\n",
      "           - Delivery: Reserved or doubtful language.\n",
      "           - Consistency: Low conviction if the title makes a bold claim, but the transcript lacks matching conviction.\n",
      "         - \"2\" (Moderate Conviction):\n",
      "           - Tone: Relatively confident language with some qualifiers.\n",
      "           - Delivery: Balanced and moderately positive language.\n",
      "           - Consistency: Medium conviction if the title makes a bold claim, followed by consistent confidence in the transcript.\n",
      "         - \"3\" (High Conviction):\n",
      "           - Tone: Strong, assertive language without hesitation.\n",
      "           - Delivery: Decisive recommendations with no qualifiers.\n",
      "           - Consistency: High conviction if the title and transcript are strongly aligned.\n",
      "       - `Ticker Name`: Specify the ticker name of the stock being discussed.\n",
      "    \n",
      "    3. If `Stock Recommendations Present` is \"No\", return the following structure:{\"Stock Recommendations Present\": \"No\",\n",
      "         \"Recommendations\": []\n",
      "       }\n",
      "    \n",
      "    \n",
      "    Output Requirements:\n",
      "    - Return only valid JSON that can be directly parsed by JSON libraries.\n",
      "    - Do not include any additional text, comments, formatting indicators (e.g., `json` or backticks), or explanatory content.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference All for Given Model (Whole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to run for ```whole``` models CHANGE THE PROMPT parameter, output path, and check above prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_lms = {\"claude-3-opus-20240229\": \"Anthropic\", \n",
    "           \"claude-3-5-haiku-20241022\": \"Anthropic\", \n",
    "           \"claude-3-5-sonnet-20241022\": \"Anthropic\",\n",
    "           \"deepseek-ai/DeepSeek-R1\": \"Together AI\",\n",
    "           \"deepseek-ai/DeepSeek-V3\": \"Together AI\",\n",
    "           \"gemini-1.5-pro-002\": \"Gemini\",\n",
    "           \"gpt-4o-2024-08-06\": \"OpenAI\",\n",
    "           \"gpt-4o-mini-2024-07-18\": \"OpenAI\",\n",
    "           \"gpt-4-turbo-2024-04-09\": \"OpenAI\",\n",
    "           \"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\": \"Together AI\",\n",
    "           \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\": \"Together AI\",\n",
    "           \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\": \"Together AI\",\n",
    "           \"Qwen/Qwen2.5-72B-Instruct-Turbo\": \"Together AI\",\n",
    "           \"Qwen/Qwen2.5-7B-Instruct-Turbo\": \"Together AI\",\n",
    "           \"mistralai/Mistral-7B-Instruct-v0.2\": \"Together AI\",\n",
    "           \"mistralai/Mixtral-8x7B-Instruct-v0.1\": \"Together AI\",\n",
    "           \"mistralai/Mixtral-8x22B-Instruct-v0.1\": \"Together AI\",\n",
    "          }\n",
    "\n",
    "DID NOT RUN DEEKSEEK \n",
    "\n",
    "\"deepseek-ai/DeepSeek-R1\": \"Together AI\",\n",
    "\n",
    "           \"deepseek-ai/DeepSeek-V3\": \"Together AI\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## whole\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"../data_needed_for_inference/w_transcripts_for_inference.csv\")\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['video_id', 'video_title', 'transcript'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-opus-20240229\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "claude-3-5-haiku-20241022\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "claude-3-5-sonnet-20241022\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "deepseek-ai/DeepSeek-R1\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "deepseek-ai/DeepSeek-V3\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "gemini-1.5-pro-002\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "gpt-4o-2024-08-06\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "gpt-4o-mini-2024-07-18\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "gpt-4-turbo-2024-04-09\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "Qwen/Qwen2.5-72B-Instruct-Turbo\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n",
      "mistralai/Mixtral-8x22B-Instruct-v0.1\n",
      "Processed 50 rows out of 288\n",
      "Processed 100 rows out of 288\n",
      "Processed 150 rows out of 288\n",
      "Processed 200 rows out of 288\n",
      "Processed 250 rows out of 288\n"
     ]
    }
   ],
   "source": [
    "all_lms = {\"claude-3-opus-20240229\": \"Anthropic\", \n",
    "           \"claude-3-5-haiku-20241022\": \"Anthropic\", \n",
    "           \"claude-3-5-sonnet-20241022\": \"Anthropic\",\n",
    "           \"deepseek-ai/DeepSeek-R1\": \"Together AI\",\n",
    "           \"deepseek-ai/DeepSeek-V3\": \"Together AI\",\n",
    "           \"gemini-1.5-pro-002\": \"Gemini\",\n",
    "           \"gpt-4o-2024-08-06\": \"OpenAI\",\n",
    "           \"gpt-4o-mini-2024-07-18\": \"OpenAI\",\n",
    "           \"gpt-4-turbo-2024-04-09\": \"OpenAI\",\n",
    "           \"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\": \"Together AI\",\n",
    "           \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\": \"Together AI\",\n",
    "           \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\": \"Together AI\",\n",
    "           \"Qwen/Qwen2.5-72B-Instruct-Turbo\": \"Together AI\",\n",
    "           \"Qwen/Qwen2.5-7B-Instruct-Turbo\": \"Together AI\",\n",
    "           \"mistralai/Mistral-7B-Instruct-v0.1\": \"Together AI\",\n",
    "           \"mistralai/Mixtral-8x7B-Instruct-v0.1\": \"Together AI\",\n",
    "           \"mistralai/Mixtral-8x22B-Instruct-v0.1\": \"Together AI\",\n",
    "          }\n",
    "\n",
    "# Loop through each model\n",
    "for model_name, model_source in all_lms.items():\n",
    "    print(model_name)\n",
    "\n",
    "    # Prepare a DataFrame to store results\n",
    "    output_df = pd.DataFrame(columns=[\n",
    "        'video_id',\n",
    "        'video_title',\n",
    "        'transcript',\n",
    "        'model_source',\n",
    "        'model_name',\n",
    "        'date_of_inference',\n",
    "        'prompt_used',\n",
    "        'prompt_output',\n",
    "        'video_source'\n",
    "    ])\n",
    "    \n",
    "\n",
    "    # Initialize a counter\n",
    "    iteration_count = 0\n",
    "    \n",
    "    # Loop through each row in the DataFrame\n",
    "    # video_source will need to be modified \n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        # Increment the counter\n",
    "        iteration_count += 1\n",
    "    \n",
    "        # Print progress every 50 iterations\n",
    "        if iteration_count % 50 == 0:\n",
    "            print(f\"Processed {iteration_count} rows out of {len(df)}\")\n",
    "        \n",
    "        # pandas series to put into model\n",
    "        series = row\n",
    "\n",
    "        prompt = create_prompt(series = series,\n",
    "                  lm_type = 'lm',\n",
    "                  whole = True,\n",
    "                  price = False\n",
    "                 )\n",
    "\n",
    "        \n",
    "        prompt_output = inference_model(prompt = prompt,\n",
    "                        model_source =  model_source,\n",
    "                        model_name = model_name,\n",
    "                        temperature = 0,)\n",
    "        \n",
    "        \n",
    "        # Append results to output DataFrame using pd.concat\n",
    "        new_row = pd.DataFrame([{\n",
    "            'video_id': series['video_id'],\n",
    "            'video_title': series['video_title'],\n",
    "            'transcript': series['transcript'], \n",
    "            'model_source': model_source,\n",
    "            'model_name': model_name,\n",
    "            'date_of_inference': datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "            'prompt_used': prompt,\n",
    "            'prompt_output': prompt_output,\n",
    "            'video_source': None\n",
    "        }])\n",
    "    \n",
    "        output_df = pd.concat([output_df, new_row], ignore_index=True)\n",
    "    \n",
    "    # Save the output for this model to a separate CSV file\n",
    "    today = datetime.now()\n",
    "    output_df.to_csv(\n",
    "        f'llm_prompt_w_transcript_outputs/{model_name.replace(\"/\", \"_\")}_{today.strftime(\"%d_%m_%Y\")}.csv',\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For segments, i dont output the ```segment_transcript```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df # concat date of inference to file name when exporting to csv \"<date>____.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
