{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab2cfec5571b433f8f9b6b62d9c003ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd385b9691ec4f5581aa21fc854237ea",
              "IPY_MODEL_814ee1c6962f42cf9f717c85d61621d8",
              "IPY_MODEL_77bc9c0b9f554b4c894e6656a75e47f2"
            ],
            "layout": "IPY_MODEL_93e16eacd56b49159000b3b97cd2cd85"
          }
        },
        "fd385b9691ec4f5581aa21fc854237ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e78ace687a624da38b0f62f512d90a70",
            "placeholder": "​",
            "style": "IPY_MODEL_00a159c436d14c28b1fc8843b307776b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "814ee1c6962f42cf9f717c85d61621d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cdd68f9e26a46f7b426a2a31b21cf83",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2da20461303494e88f95c4e76f5cab3",
            "value": 3
          }
        },
        "77bc9c0b9f554b4c894e6656a75e47f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08b66183a95d451f973bc3cc42ce44c5",
            "placeholder": "​",
            "style": "IPY_MODEL_0fefcb90c7c54d5eae9e1ba4e1df8a09",
            "value": " 3/3 [00:05&lt;00:00,  1.73s/it]"
          }
        },
        "93e16eacd56b49159000b3b97cd2cd85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e78ace687a624da38b0f62f512d90a70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00a159c436d14c28b1fc8843b307776b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cdd68f9e26a46f7b426a2a31b21cf83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2da20461303494e88f95c4e76f5cab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08b66183a95d451f973bc3cc42ce44c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fefcb90c7c54d5eae9e1ba4e1df8a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-BjIQQGN_A-",
        "outputId": "abafacaa-9558-40a6-e802-7f8bd253e860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: decord in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision transformers decord\n",
        "!pip install numpy pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObILEjP1OAx1",
        "outputId": "7e864193-e241-4a6d-d223-efcf7a5dbdb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Research/YoutubePortfolio/Benchmarking"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-Tl3z1lOC-K",
        "outputId": "dbe10c48-56ae-4b8a-a98c-0da2a7bb4c94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Research/YoutubePortfolio/Benchmarking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from transformers import LlavaNextVideoProcessor, LlavaNextVideoForConditionalGeneration\n",
        "import torch\n",
        "from decord import VideoReader, cpu\n",
        "from torchvision import transforms\n",
        "from decord import VideoReader, cpu\n",
        "from torchvision.transforms import Resize, ToPILImage, Compose\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "hrCiRP_YOERd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = './video_transcriptions.csv'\n",
        "video_segments_path = './video_segments'\n",
        "output_dir='./model_outputs_video_segments'\n",
        "\n",
        "frame_rate = 0.25  # frames per second\n",
        "max_frame_size = 512  # Maximum size for frame resizing\n",
        "\n",
        "# This works\n",
        "#model_id = \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\n",
        "#model_id = \"llava-hf/LLaVA-NeXT-Video-7B-32K-hf\"\n",
        "#model_id = \"llava-hf/LLaVA-NeXT-Video-7B-DPO-hf\"\n",
        "model_id = \"llava-hf/LLaVA-NeXT-Video-34B-hf\"\n",
        "#model_id = \"llava-hf/LLaVA-NeXT-Video-34B-DPO-hf\""
      ],
      "metadata": {
        "id": "4Tfr7pF5OE7a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_llava_next_model():\n",
        "    #token = \"\n",
        "    processor = LlavaNextVideoProcessor.from_pretrained(model_id)\n",
        "    model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "    ).to(0)\n",
        "\n",
        "    return processor, model\n",
        "\n",
        "def query_llava_next(processor, model, prompt, input_frames):\n",
        "    if input_frames is None or input_frames.shape[0] == 0:\n",
        "        raise ValueError(\"No frames provided for processing.\")\n",
        "    inputs_video = processor(text=prompt, videos=input_frames, padding=True, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    output = model.generate(**inputs_video, max_new_tokens=2048)\n",
        "    return processor.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "tMKm__03OhpA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(series, lm_type = \"lm\", whole = True, price = False):\n",
        "    \"\"\"\n",
        "    Generates a structured prompt based on the provided title, transcript, language model type, and whether we.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): The pandas series where we get title, transcript, segment_transcript\n",
        "        processor: For VLM specifically\n",
        "        lm_type (str, optional): The type of language model the prompt is intended for (default is \"lm\").\n",
        "                                 Options are \"lm\" or \"vlm\". vlm has a slightly larger prompt to incorporate facial expression.\n",
        "        whole (bool, optional): Specifies whether to use the entire transcript (True) or a subset (False).\n",
        "                                Defaults to True.\n",
        "        price (bool, optional): Only available for vlm. Can the vlm detect price.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted prompt stored in the variable `prompt`, ready for input into a language model.\n",
        "    \"\"\"\n",
        "\n",
        "    # title\n",
        "    # transcript\n",
        "    # 'segment_transcript'\n",
        "\n",
        "    title = series['video_title']\n",
        "    transcript = series['transcript']\n",
        "    segment_transcript = series['segment_transcript']\n",
        "\n",
        "    # The \"video is provided along\" part of the prompt is from hour video\n",
        "    # https://huggingface.co/datasets/HourVideo/HourVideo/blob/main/prompts/baseline_evaluations/gemini-1.5-pro/qa_eval.yaml\n",
        "    if lm_type == \"vlm\":\n",
        "        video_statement = (\"\\nThe video is provided along with this prompt.\",)\n",
        "    else:\n",
        "        video_statement = (\"\",)\n",
        "\n",
        "    if lm_type == \"vlm\" and whole == True:\n",
        "        yt_video_statement = \"video, \"\n",
        "    elif lm_type == \"vlm\" and whole == False:\n",
        "        yt_video_statement = \"video and \"\n",
        "    else:\n",
        "        yt_video_statement = \"\"\n",
        "\n",
        "    if lm_type == \"vlm\":\n",
        "        facial_expression = (\"\\n           - Facial Expressions: Neutral or doubtful (furrowed brows, pursed lips).\",\n",
        "                             \"\\n           - Facial Expressions: Moderate enthusiasm (mild smiles, slightly raised eyebrows).\",\n",
        "                             \"\\n           - Facial Expressions: Enthusiastic, energetic (wide smiles, raised eyebrows).\")\n",
        "    else:\n",
        "        facial_expression = (\"\",\n",
        "                             \"\",\n",
        "                             \"\")\n",
        "\n",
        "    if whole == True and lm_type == \"vlm\":\n",
        "        whole_transcript_specific = (\", and video title\",\n",
        "                            f\"\"\"Inputs:\n",
        "    - Video Title: {title}\n",
        "    - Transcript: {transcript}\"\"\",\n",
        "                            \"\\n           - Consistency: Low conviction if the title makes a bold claim, but the transcript lacks matching conviction.\",\n",
        "                            \"\\n           - Consistency: Medium conviction if the title makes a bold claim, followed by consistent confidence in the transcript.\",\n",
        "                            \"\\n           - Consistency: High conviction if the title and transcript are strongly aligned.\")\n",
        "    elif whole == True and lm_type == \"lm\":\n",
        "      whole_transcript_specific = (\" and video title\",\n",
        "                          f\"\"\"Inputs:\n",
        "  - Video Title: {title}\n",
        "  - Transcript: {transcript}\"\"\",\n",
        "                          \"\\n           - Consistency: Low conviction if the title makes a bold claim, but the transcript lacks matching conviction.\",\n",
        "                          \"\\n           - Consistency: Medium conviction if the title makes a bold claim, followed by consistent confidence in the transcript.\",\n",
        "                          \"\\n           - Consistency: High conviction if the title and transcript are strongly aligned.\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        whole_transcript_specific = (\"\",\n",
        "                            f\"\"\"Inputs:\n",
        "    - Transcript: {segment_transcript}\"\"\",\n",
        "                             \"\",\n",
        "                             \"\",\n",
        "                            \"\")\n",
        "\n",
        "    # VLM part of this prompt \"video provided along with this prompt\" was inspired by hour video\n",
        "    # https://huggingface.co/datasets/HourVideo/HourVideo/blob/main/prompts/baseline_evaluations/gemini-1.5-pro/qa_eval.yaml\n",
        "\n",
        "    prompt = f\"\"\"Analyze the YouTube {yt_video_statement}transcript{whole_transcript_specific[0]} of influencers discussing the US stock market, focusing on stock recommendations and their conviction.{video_statement[0]}\n",
        "\n",
        "    {whole_transcript_specific[1]}\n",
        "\n",
        "    Instructions:\n",
        "    1. Does the video contain any stock recommendations:\n",
        "       - Label this as `Stock Recommendations Present` with either \"Yes\" or \"No\".\n",
        "\n",
        "    2. If `Stock Recommendations Present` is \"Yes\", create a list under the key `Recommendations`. Each recommendation should follow this structure:{{\"Action\": \"Buy | Hold | Don't Buy | Sell | Short Sell | Unclear\",\n",
        "         \"Justification\": \"Brief explanation for the action based on the transcript\",\n",
        "         \"Conviction Score\": \"1 | 2 | 3\",\n",
        "         \"Ticker Name\": \"Ticker name\"}}\n",
        "\n",
        "       Details for each field:\n",
        "       - `Action`: Categorize each stock recommendation as:\n",
        "         - \"Buy\": Purchase shares of the stock.\n",
        "         - \"Hold\":  Retain the stock if already owned, without necessarily\n",
        "    buying more.\n",
        "         - \"Don't Buy\": Refrain from purchasing the stock.\n",
        "         - \"Sell\": Sell shares of the stock currently owned.\n",
        "         - \"Short Sell\": Sell shares not currently owned, intending to\n",
        "    buy them back later at a lower price.\n",
        "         - \"Unclear\": When the action is not explicitly stated.\n",
        "       - `Justification`: Provide a brief explanation for the action based on the transcript.\n",
        "       - `Conviction Score`: Assign a score based on the following criteria:\n",
        "         - \"1\" (Low Conviction):\n",
        "           - Tone: Hesitant or uncertain language, frequent qualifiers (e.g., “maybe,” “possibly”).{facial_expression[0]}\n",
        "           - Delivery: Reserved or doubtful language.{whole_transcript_specific[2]}\n",
        "         - \"2\" (Moderate Conviction):\n",
        "           - Tone: Relatively confident language with some qualifiers.{facial_expression[1]}\n",
        "           - Delivery: Balanced and moderately positive language.{whole_transcript_specific[3]}\n",
        "         - \"3\" (High Conviction):\n",
        "           - Tone: Strong, assertive language without hesitation.{facial_expression[2]}\n",
        "           - Delivery: Decisive recommendations with no qualifiers.{whole_transcript_specific[4]}\n",
        "       - `Ticker Name`: Specify the ticker name of the stock being discussed.\n",
        "\n",
        "    3. If `Stock Recommendations Present` is \"No\", return the following structure:{{\"Stock Recommendations Present\": \"No\",\n",
        "         \"Recommendations\": []\n",
        "       }}\n",
        "\n",
        "    Output Requirements:\n",
        "    - Return only valid JSON that can be directly parsed by JSON libraries.\n",
        "    - Do not include any additional text, comments, formatting indicators (e.g., `json` or backticks), or explanatory content.\n",
        "    \"\"\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "UXLe6O07RXlW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video(video_path, sampling_fps, size=512, verbose=True):\n",
        "    \"\"\"\n",
        "    Load video frames, sample them at the specified frame rate, and return a NumPy array of raw RGB frames.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the video file.\n",
        "        sampling_fps (float): Frames per second to sample.\n",
        "        size (int): The maximum size (in pixels) of the larger dimension for resizing.\n",
        "        verbose (bool): Whether to print debug information.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Decoded and sampled frames as a NumPy array of shape (num_frames, height, width, 3).\n",
        "    \"\"\"\n",
        "    # Initialize the video reader\n",
        "    video_reader = VideoReader(video_path, ctx=cpu(0))\n",
        "    video_length = len(video_reader)\n",
        "\n",
        "    # Get the frames per second (FPS)\n",
        "    fps = video_reader.get_avg_fps()\n",
        "\n",
        "    # Determine the indices of frames to sample\n",
        "    frame_indices = np.arange(0, video_length, int(fps / sampling_fps))\n",
        "    if verbose:\n",
        "        print(f'> Reading video: {video_path}')\n",
        "        print(f'Stats => fps: {fps}, #frames: {video_length}, sampling fps: {sampling_fps}, #sampled_frames: {len(frame_indices)}')\n",
        "\n",
        "    # Extract raw frames as numpy arrays\n",
        "    raw_sample_frms = video_reader.get_batch(frame_indices).asnumpy()  # Shape: (Batch, Height, Width, Channels)\n",
        "    if verbose:\n",
        "        print(f'Raw frames shape before resizing: {raw_sample_frms.shape}')\n",
        "\n",
        "    # Resize frames while maintaining aspect ratio\n",
        "    def resize_frame(frame, target_size):\n",
        "        from PIL import Image\n",
        "        image = Image.fromarray(frame)  # Convert to PIL Image\n",
        "        width, height = image.size\n",
        "\n",
        "        # Calculate the new dimensions\n",
        "        if width > height:\n",
        "            new_width = target_size\n",
        "            new_height = int((target_size / width) * height)\n",
        "        else:\n",
        "            new_height = target_size\n",
        "            new_width = int((target_size / height) * width)\n",
        "\n",
        "        # Resize the image and convert back to NumPy array\n",
        "        resized_image = image.resize((new_width, new_height), Image.BICUBIC)\n",
        "        return np.array(resized_image)\n",
        "\n",
        "    # Resize all sampled frames\n",
        "    processed_frames = np.array([resize_frame(frame, size) for frame in raw_sample_frms])\n",
        "    if verbose:\n",
        "        print(f'Processed frames shape after resizing: {processed_frames.shape}')\n",
        "\n",
        "    return processed_frames"
      ],
      "metadata": {
        "id": "W63rcUiSTZRK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_response(video_id, start, end, response, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
        "    file_path = os.path.join(output_dir, f\"{video_id}__{start}__{end}.txt\")\n",
        "    with open(file_path, \"w\") as f:\n",
        "        f.write(response)\n",
        "\n",
        "def load_existing_response(video_id, start, end, output_dir):\n",
        "    file_path = os.path.join(output_dir, f\"{video_id}__{start}__{end}.txt\")\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, \"r\") as f:\n",
        "            return f.read()\n",
        "    return None"
      ],
      "metadata": {
        "id": "AdD2qfF9hmd9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_row(row, processor, model, frame_rate, max_frame_size, video_segments_path, output_dir):\n",
        "    video_id = row['video_id']\n",
        "    title = row['video_title']\n",
        "    start = row['start']\n",
        "    end = row['end']\n",
        "    segment_transcript = row['segment_transcript']\n",
        "\n",
        "    # Check if output already exists\n",
        "    existing_response = load_existing_response(video_id, start, end, output_dir)\n",
        "    if existing_response:\n",
        "        print(f\"Skipping row as output already exists: {video_id}__{start}__{end}\")\n",
        "        return existing_response\n",
        "\n",
        "    # Skip if segment duration exceeds 4 minutes\n",
        "    if (end - start) > 150:\n",
        "        print(f\"Skipping row due to segment duration exceeding 4 minutes: {video_id}__{start}__{end}\")\n",
        "        return \"VideoTooLong\"\n",
        "\n",
        "    video_file_path = os.path.join(video_segments_path, f\"{video_id}__{start}__{end}.mp4\")\n",
        "    if not os.path.exists(video_file_path):\n",
        "        print(f\"Video file not found: {video_file_path}\")\n",
        "        return \"NoVideoFile\"\n",
        "\n",
        "    try:\n",
        "        # Extract frames\n",
        "        frames = load_video(video_file_path, sampling_fps=frame_rate, size=max_frame_size, verbose=True)\n",
        "        if frames is None or len(frames) == 0:\n",
        "            print(f\"No frames extracted for video: {video_file_path}\")\n",
        "            return \"NoFrames\"\n",
        "\n",
        "        # Create prompt\n",
        "        num_images = len(frames)\n",
        "        # Whole needs to be changed to True for Full Videos (there might be one or two videos we can run)\n",
        "        base_prompt = create_prompt(row, lm_type = \"vlm\", whole = False, price = False)\n",
        "\n",
        "        # VLM Specific\n",
        "        conversation = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "              {\"type\": \"text\", \"text\": base_prompt},\n",
        "              {\"type\": \"video\"},\n",
        "            ],\n",
        "          },\n",
        "        ]\n",
        "        prompt = processor.apply_chat_template(conversation)\n",
        "\n",
        "\n",
        "        # Query LLaVA-NeXT model\n",
        "        response = query_llava_next(processor, model, prompt, frames)\n",
        "\n",
        "        # Save response\n",
        "        save_response(video_id, start, end, response, output_dir)\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_id}__{start}__{end}: {e}\")\n",
        "        return \"ErrorWhileProcessing\""
      ],
      "metadata": {
        "id": "rHg8Iy6pQYrE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_videos(df, processor, model, video_segments_path, frame_rate, max_frame_size, output_dir):\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Iterate over each row\n",
        "    for index, row in df.iterrows():\n",
        "        try:\n",
        "            response = process_video_row(row, processor, model, frame_rate, max_frame_size, video_segments_path, output_dir)\n",
        "            print(f\"Response for row {index}: {response}\")\n",
        "            results.append(response)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing row {index}: {e}\")\n",
        "            results.append(None)\n",
        "\n",
        "    # Add model outputs to DataFrame\n",
        "    df['llava_next_segment_output'] = results\n",
        "\n",
        "    # Save updated DataFrame\n",
        "    output_csv_path = \"llava_next_segment.csv\"\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"Updated CSV saved to {output_csv_path}\")"
      ],
      "metadata": {
        "id": "DTQEVZTTTKW-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b9R4P4OkO9oO",
        "outputId": "d78f543d-cdc5-48ea-f9c1-c9bb1c83e807"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./video_transcriptions.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df = pd.read_csv(data_path).head(1)\n",
        "\n",
        "# Load processor and model\n",
        "processor, model = load_llava_next_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "ab2cfec5571b433f8f9b6b62d9c003ef",
            "fd385b9691ec4f5581aa21fc854237ea",
            "814ee1c6962f42cf9f717c85d61621d8",
            "77bc9c0b9f554b4c894e6656a75e47f2",
            "93e16eacd56b49159000b3b97cd2cd85",
            "e78ace687a624da38b0f62f512d90a70",
            "00a159c436d14c28b1fc8843b307776b",
            "9cdd68f9e26a46f7b426a2a31b21cf83",
            "a2da20461303494e88f95c4e76f5cab3",
            "08b66183a95d451f973bc3cc42ce44c5",
            "0fefcb90c7c54d5eae9e1ba4e1df8a09"
          ]
        },
        "id": "EgSojaqNgpaa",
        "outputId": "a6b6d50b-f2bf-4910-8562-3cc444653ba8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab2cfec5571b433f8f9b6b62d9c003ef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsZj4G-ngi57",
        "outputId": "bb3cc75e-3b9a-41a9-a605-81bf5a5629bc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'id', 'derived_inner_id', 'video_id', 'start', 'end',\n",
              "       'action', 'action_source', 'conviction_score', 'ticker_name',\n",
              "       'action_date', 'price', 'quantity', 'video_title', 'annotation_id',\n",
              "       'annotator', 'is_rec_present', 'original_inner_id',\n",
              "       'original_video_title', 'publishedAt', 'channelId', 'channelTitle',\n",
              "       'videoDescription', 'tags', 'defaultAudioLanguage', 'duration',\n",
              "       'isCaptionAvailable', 'viewCount', 'likeCount', 'favoriteCount',\n",
              "       'commentCount', 'comments', 'channelDescription', 'channelViewCount',\n",
              "       'channelSubscriberCount', 'videoCount', 'channelCategory', 'transcript',\n",
              "       'segment_transcript'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fzNdMwHWwOEt",
        "outputId": "dfef375a-6d41-454e-d34c-f8f90e183748"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./model_outputs_video_segments'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df, video_segments_path, frame_rate, max_frame_size, output_dir\n",
        "\n",
        "process_videos(df = df,\n",
        "               processor = processor,\n",
        "               model = model,\n",
        "               video_segments_path = video_segments_path,\n",
        "               frame_rate = frame_rate,\n",
        "               max_frame_size = max_frame_size,\n",
        "               output_dir = output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdjLu75sgi0U",
        "outputId": "10c567b1-b938-4042-bd29-8abb8b6a3832"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Reading video: ./video_segments/0CJU8R4oNFk__109.87117723384252__158.5389470152761.mp4\n",
            "Stats => fps: 23.976023976023978, #frames: 1154, sampling fps: 0.25, #sampled_frames: 13\n",
            "Raw frames shape before resizing: (13, 720, 1280, 3)\n",
            "Processed frames shape after resizing: (13, 288, 512, 3)\n",
            "Response for row 0: USER: \n",
            "Analyze the YouTube video and transcript of influencers discussing the US stock market, focusing on stock recommendations and their conviction.\n",
            "The video is provided along with this prompt.\n",
            "    \n",
            "    Inputs:\n",
            "    - Transcript:  these in a minute. First up is $225 million dollar Veritone Inc., ticker V-E-R-I, a cloud-based AI platform that structures audio and video data. And now Nation, if that sounds like a bunch of tech jargon, just understand that Veritone is in the convergence of what's going to be the three biggest trends over the next decade. Data analysis of audio and video content, cloud-based connectivity, and an AI platform that learns to become more effective. And the potential markets for these speak for themselves. The company's AIware platform is facing markets with double-digit annual growth and tens of billions in opportunities. Revenue has jumped 244% in the last two years and was even able to increase in that tough second quarter. Analysts have price targets ranging from $15 a share to as high as $18 each over the next year for a potential 120% return.\n",
            "    \n",
            "    Instructions:\n",
            "    1. Does the video contain any stock recommendations:\n",
            "       - Label this as `Stock Recommendations Present` with either \"Yes\" or \"No\".\n",
            "    \n",
            "    2. If `Stock Recommendations Present` is \"Yes\", create a list under the key `Recommendations`. Each recommendation should follow this structure:{\"Action\": \"Buy | Hold | Don't Buy | Sell | Short Sell | Unclear\",\n",
            "         \"Justification\": \"Brief explanation for the action based on the transcript\",\n",
            "         \"Conviction Score\": \"1 | 2 | 3\",\n",
            "         \"Ticker Name\": \"Ticker name\"}\n",
            "    \n",
            "       Details for each field:\n",
            "       - `Action`: Categorize each stock recommendation as:\n",
            "         - \"Buy\": Purchase shares of the stock.\n",
            "         - \"Hold\":  Retain the stock if already owned, without necessarily\n",
            "    buying more.\n",
            "         - \"Don't Buy\": Refrain from purchasing the stock.\n",
            "         - \"Sell\": Sell shares of the stock currently owned.\n",
            "         - \"Short Sell\": Sell shares not currently owned, intending to\n",
            "    buy them back later at a lower price.\n",
            "         - \"Unclear\": When the action is not explicitly stated.\n",
            "       - `Justification`: Provide a brief explanation for the action based on the transcript.\n",
            "       - `Conviction Score`: Assign a score based on the following criteria:\n",
            "         - \"1\" (Low Conviction):\n",
            "           - Tone: Hesitant or uncertain language, frequent qualifiers (e.g., “maybe,” “possibly”).\n",
            "           - Facial Expressions: Neutral or doubtful (furrowed brows, pursed lips).\n",
            "           - Delivery: Reserved or doubtful language.\n",
            "         - \"2\" (Moderate Conviction):\n",
            "           - Tone: Relatively confident language with some qualifiers.\n",
            "           - Facial Expressions: Moderate enthusiasm (mild smiles, slightly raised eyebrows).\n",
            "           - Delivery: Balanced and moderately positive language.\n",
            "         - \"3\" (High Conviction):\n",
            "           - Tone: Strong, assertive language without hesitation.\n",
            "           - Facial Expressions: Enthusiastic, energetic (wide smiles, raised eyebrows).\n",
            "           - Delivery: Decisive recommendations with no qualifiers.\n",
            "       - `Ticker Name`: Specify the ticker name of the stock being discussed.\n",
            "    \n",
            "    3. If `Stock Recommendations Present` is \"No\", return the following structure:{\"Stock Recommendations Present\": \"No\",\n",
            "         \"Recommendations\": []\n",
            "       }\n",
            "    \n",
            "    Output Requirements:\n",
            "    - Return only valid JSON that can be directly parsed by JSON libraries.\n",
            "    - Do not include any additional text, comments, formatting indicators (e.g., `json` or backticks), or explanatory content.\n",
            "      - Each recommendation should be a JSON object with the following structure: {\"Action\": \"Buy | Hold | Don't Buy | Sell | Short Sell | Unclear\",\n",
            "         \"Justification\": \"Brief explanation for the action based on the transcript\",\n",
            "         \"Conviction Score\": \"1 | 2 | 3\",\n",
            "         \"Ticker Name\": \"Ticker name\"}\n",
            "\n",
            "\n",
            "\n",
            "Example:\n",
            "{\"Stock Recommendations Present\": \"Yes\",\n",
            "\"Recommendations\": [\n",
            "{\"Action\": \"Buy\",\n",
            "\"Justification\": \"The company is growing rapidly and has a strong AI platform that is in high demand.\",\n",
            "\"Conviction Score\": \"3\",\n",
            "\"Ticker Name\": \"Veritone Inc.\"},\n",
            "{\"Action\": \"Hold\",\n",
            "\"Justification\": \"The company has been performing well and is a good long-term hold.\",\n",
            "\"Conviction Score\": \"2\",\n",
            "\"Ticker Name\": \"Veritone Inc.\"},\n",
            "{\"Action\": \"Sell\",\n",
            "\"Justification\": \"The stock has been overvalued and it's time to sell.\",\n",
            "\"Conviction Score\": \"1\",\n",
            "\"Ticker Name\": \"Veritone Inc.\"}\n",
            "]\n",
            "Updated CSV saved to llava_next_segment.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "2ke7V1SEUCg8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mx9V81e0VaKP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}